{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP ANALYSE DE SENTIMENT - PARTIE LAPTOPS\n",
    "Le jeu de données Laptop_Train_v0.2 est composé de 3045 phrases anglaises extraites des  commentaires  des  clients  sur  les  ordinateurs  portables.  \n",
    "Des  annotateurs  humains expérimentés ont annoté les termes d'aspect des phrases et leurs polarités.\n",
    "Les valeurs possibles pour la polarité des aspects sont : “positive”, “negative”, “conflict”, “neutral”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données Laptop_Train est composé de 3045 phrases anglaises extraites \n",
    "des commentaires des clients sur les ordinateurs portables. Des annotateurs humains \n",
    "expérimentés ont annoté les termes d'aspect des phrases et leurs polarités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première tâche : analyse des sentiments du jeux de données sur les restaurants \n",
    "On commence par télécharger les fichiers suivants qui se trouvent dans le repertoire TPSA/datasets sur Moodle :\n",
    "* Laptop_Train.xml (ensemble d'entrainement)\n",
    "* Laptop_Test_Gold.xml (ensemble de test)\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Objectif 1: calculer la polarité des mots dans les deux jeux de données à l’aide d’un lexicon de sentiment. \n",
    "Avant d’utiliser le lexicon pour calculer la polarité des mots contenus dans les phrases dans les deux jeux de données, il est nécessaire de faire un pré-traitement des phases (negation, tokenizer, PoS tagger, et NER). \n",
    "* Vous devez implémenter un système d'extraction d'informations simple. Le texte brut de chaque phrase est subdivisée en mots à l'aide d'un tokenizer. \n",
    "* Ensuite, chaque phrase est étiquetée avec des balises de partie de discours (PoS tagger), ~~ce qui s'avérera très utile à l'étape suivante, la détection d'entités nommées (NER).~~\n",
    "\n",
    "**Remarque**\n",
    "- La NER n'est pas nécessaire ici, car nous avons déjà les terms à identifier dans le Train et le test. \n",
    "- Les étapes à suivres sont alors les suivantes :\n",
    "\n",
    "* **Chargement**\n",
    "    * Charger les données depuis les fichiers en récupérant le terme et toutes ses informations (le term lui meme, son emplacement, sa phrase, sa polarité)\n",
    "    * Transformer les phrases en token avec PosTag afin de les ajouter dans notre dataframe et d'avoir des infos supplémentaires sur la structure de la phrase.\n",
    "* **Sentiments**\n",
    "    * Une  fois  que  le  pré-traitement  des  phrases  est  terminé,  vous  pouvez  télécharger  le lexicon SentiWordNet (https://github.com/aesuli/SentiWordNet).\n",
    "    * Apres le téléchargement, vous devez identifier la polarité associé a chaque mot dans les phrases contenues dans les jeux de données (fichiers Train et Test, 4 fichiers à traiter) en utilisant le lexicon SentiWordNet:\n",
    "     - pour chaque mot (que vous avez identifié avec le tokenizer, stop words exclues) vous cherchez si le mot est present dans le lexicon. \n",
    "     - S’il est present, alors vous assignez à ce mot la polarité positive/negative associée au mot dans le lexicon ansi que le degré associé.A vous de choisir le format (balises) pour stocker ces informations, qui vous seront utiles après. \n",
    "     - S’il n’est pas present, vous pouvez passer au mot suivant.\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier.\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\belka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#SPACY\n",
    "import en_core_web_sm   # téléchargement ici : https://spacy.io/models/en\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "#AUTRES LIBRAIRIES\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### **CHARGEMENT**\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs possibles pour la polarité des aspects sont : “positive”, “negative”, “conflict”, “neutral”. \n",
    "Les valeurs possibles des categories sont : “food”, “service”, “price”, “ambience”, “anecdotes/miscellaneous”.\n",
    "* Nous ne nous intéressons qu'au polarité des terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>109</td>\n",
       "      <td>121</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech guy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from   to    id  \\\n",
       "0            cord   neutral   41   45  2339   \n",
       "1    battery life  positive   74   86  2339   \n",
       "2  service center  negative   27   41  1316   \n",
       "3    \"sales\" team  negative  109  121  1316   \n",
       "4        tech guy   neutral    4   12  1316   \n",
       "\n",
       "                                                text  \n",
       "0  I charge it at night and skip taking the cord ...  \n",
       "1  I charge it at night and skip taking the cord ...  \n",
       "2  The tech guy then said the service center does...  \n",
       "3  The tech guy then said the service center does...  \n",
       "4  The tech guy then said the service center does...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- TRAIN DATAFRAME ----- #\n",
    "l = []\n",
    "\n",
    "xtree = et.parse(\"Laptop_Train.xml\")\n",
    "xroot = xtree.getroot()\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_train = pd.DataFrame(l)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2128</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>costing</td>\n",
       "      <td>positive</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>655</td>\n",
       "      <td>It was truly a great computer costing less tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  polarity from  to    id  \\\n",
       "0  aluminum body  positive   12  25  2128   \n",
       "1         screen  positive   20  26    81   \n",
       "2  build quality  negative    9  22   353   \n",
       "3    performance  negative   30  41   353   \n",
       "4        costing  positive   30  37   655   \n",
       "\n",
       "                                                text  \n",
       "0                         I liked the aluminum body.  \n",
       "1           Lightweight and the screen is beautiful!  \n",
       "2  From the build quality to the performance, eve...  \n",
       "3  From the build quality to the performance, eve...  \n",
       "4  It was truly a great computer costing less tha...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- TEST DATAFRAME ----- #\n",
    "l = []\n",
    "\n",
    "xtree = et.parse(\"Laptop_Test_Gold.xml\")\n",
    "xroot = xtree.getroot()\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_test = pd.DataFrame(l)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Tokenization and PosTag\n",
    "Utilisation de en_core_web_sm de Spacy car NLTK renvoie une erreur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On recharge cette fois avec les posTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>109</td>\n",
       "      <td>121</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech guy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from   to    id  \\\n",
       "0            cord   neutral   41   45  2339   \n",
       "1    battery life  positive   74   86  2339   \n",
       "2  service center  negative   27   41  1316   \n",
       "3    \"sales\" team  negative  109  121  1316   \n",
       "4        tech guy   neutral    4   12  1316   \n",
       "\n",
       "                                                text  \\\n",
       "0  I charge it at night and skip taking the cord ...   \n",
       "1  I charge it at night and skip taking the cord ...   \n",
       "2  The tech guy then said the service center does...   \n",
       "3  The tech guy then said the service center does...   \n",
       "4  The tech guy then said the service center does...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  \n",
       "3  (The, tech, guy, then, said, the, service, cen...  \n",
       "4  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "xtree = et.parse(\"Laptop_Train.xml\")\n",
    "xroot = xtree.getroot()\n",
    "\n",
    "\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    # récupération de l'id et de la phrase\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #Tokenization du text\n",
    "    tokenize_Text = nlp(text)\n",
    "\n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        neighbor.attrib['token_text'] = tokenize_Text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_train = pd.DataFrame(l)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PRON',\n",
       " 'SCONJ',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on vérifie que les posTag ont bien été conservé\n",
    "df_train.token_text[0]\n",
    "[X.pos_ for X in  df_train.token_text[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2128</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>costing</td>\n",
       "      <td>positive</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>655</td>\n",
       "      <td>It was truly a great computer costing less tha...</td>\n",
       "      <td>(It, was, truly, a, great, computer, costing, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  polarity from  to    id  \\\n",
       "0  aluminum body  positive   12  25  2128   \n",
       "1         screen  positive   20  26    81   \n",
       "2  build quality  negative    9  22   353   \n",
       "3    performance  negative   30  41   353   \n",
       "4        costing  positive   30  37   655   \n",
       "\n",
       "                                                text  \\\n",
       "0                         I liked the aluminum body.   \n",
       "1           Lightweight and the screen is beautiful!   \n",
       "2  From the build quality to the performance, eve...   \n",
       "3  From the build quality to the performance, eve...   \n",
       "4  It was truly a great computer costing less tha...   \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  \n",
       "3  (From, the, build, quality, to, the, performan...  \n",
       "4  (It, was, truly, a, great, computer, costing, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "xtree = et.parse(\"Laptop_Test_Gold.xml\")\n",
    "xroot = xtree.getroot()\n",
    "\n",
    "\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    # récupération de l'id et de la phrase\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #Tokenization du text\n",
    "    tokenize_Text = nlp(text)\n",
    "\n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        neighbor.attrib['token_text'] = tokenize_Text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_test = pd.DataFrame(l)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### **SENTIMENTS**\n",
    "------------------------------------------------------------\n",
    "\n",
    "* Une  fois  que  le  pré-traitement  des  phrases  est  terminé,  vous  pouvez  télécharger  le lexicon SentiWordNet (https://github.com/aesuli/SentiWordNet).\n",
    "* Apres le téléchargement, vous devez identifier la polarité associé a chaque mot dans les phrases contenues dans les jeux de données (fichiers Train et Test, 4 fichiers à traiter) en utilisant le lexicon SentiWordNet:\n",
    "     - pour chaque mot (que vous avez identifié avec le tokenizer, stop words exclues) vous cherchez si le mot est present dans le lexicon. \n",
    "     - S’il est present, alors vous assignez à ce mot la polarité positive/negative associée au mot dans le lexicon ansi que le degré associé.A vous de choisir le format (balises) pour stocker ces informations, qui vous seront utiles après. \n",
    "     - S’il n’est pas present, vous pouvez passer au mot suivant.\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier. \n",
    "------------------------------------------------------------\n",
    "\n",
    "* #### Assigner la polarité chaque mot et ajouter dans la dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2128</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  polarity from  to    id  \\\n",
       "0  aluminum body  positive   12  25  2128   \n",
       "1         screen  positive   20  26    81   \n",
       "2  build quality  negative    9  22   353   \n",
       "3    performance  negative   30  41   353   \n",
       "\n",
       "                                                text Score_by_word Sentiword  \\\n",
       "0                         I liked the aluminum body.                           \n",
       "1           Lightweight and the screen is beautiful!                           \n",
       "2  From the build quality to the performance, eve...                           \n",
       "3  From the build quality to the performance, eve...                           \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  \n",
       "3  (From, the, build, quality, to, the, performan...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On ajoute deux colonnes vides à nos dataframe train et test\n",
    "#Une pour le score de chaque mot\n",
    "#df_train.insert(6,'Score_by_word',\"\")\n",
    "df_test.insert(6,'Score_by_word',\"\")\n",
    "#une pour avoir chaque mot que sentiword a détécté\n",
    "#df_train.insert(7,'Sentiword',\"\")\n",
    "df_test.insert(7,'Sentiword',\"\")\n",
    "\n",
    "\n",
    "df_test.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>109</td>\n",
       "      <td>121</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from   to    id  \\\n",
       "0            cord   neutral   41   45  2339   \n",
       "1    battery life  positive   74   86  2339   \n",
       "2  service center  negative   27   41  1316   \n",
       "3    \"sales\" team  negative  109  121  1316   \n",
       "\n",
       "                                                text Score_by_word Sentiword  \\\n",
       "0  I charge it at night and skip taking the cord ...                           \n",
       "1  I charge it at night and skip taking the cord ...                           \n",
       "2  The tech guy then said the service center does...                           \n",
       "3  The tech guy then said the service center does...                           \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  \n",
       "3  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On ajoute deux colonnes vides à nos dataframe train et test\n",
    "#Une pour le score de chaque mot\n",
    "#df_train.insert(6,'Score_by_word',\"\")\n",
    "df_train.insert(6,'Score_by_word',\"\")\n",
    "#une pour avoir chaque mot que sentiword a détécté\n",
    "#df_train.insert(7,'Sentiword',\"\")\n",
    "df_train.insert(7,'Sentiword',\"\")\n",
    "\n",
    "\n",
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ FONCTIONS POUR RECUPERATION DES SENTIMENTS DANS SENTIWORD NET ------ #\n",
    "\n",
    "# Convertion des tags en simple WORDNET TAGS\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#CHARGEMENT DES SENTIMENTS DETECTES\n",
    "# renvoie une liste de score positif negatif ou neutre et renvoie une liste vide si le mot ne renvoie rien depuis senti wordnet.\n",
    "def get_sentiment(word,tag):\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Prend le premier sens du mot, le plus commun\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>109</td>\n",
       "      <td>121</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech guy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quality</td>\n",
       "      <td>positive</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>2005</td>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>[[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...</td>\n",
       "      <td>[high, quality, ,, killer, GUI, ,, extremely, ...</td>\n",
       "      <td>(it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GUI</td>\n",
       "      <td>positive</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>2005</td>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>[[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...</td>\n",
       "      <td>[high, quality, ,, killer, GUI, ,, extremely, ...</td>\n",
       "      <td>(it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>applications</td>\n",
       "      <td>positive</td>\n",
       "      <td>118</td>\n",
       "      <td>130</td>\n",
       "      <td>2005</td>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>[[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...</td>\n",
       "      <td>[high, quality, ,, killer, GUI, ,, extremely, ...</td>\n",
       "      <td>(it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>use</td>\n",
       "      <td>positive</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>2005</td>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>[[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...</td>\n",
       "      <td>[high, quality, ,, killer, GUI, ,, extremely, ...</td>\n",
       "      <td>(it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>start up</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>2789</td>\n",
       "      <td>Easy to start up and does not overheat as much...</td>\n",
       "      <td>[[0.625, 0.25, 0.125], [], [], [0.0, 0.0, 1.0]...</td>\n",
       "      <td>[Easy, start, overheat, much, laptops, .]</td>\n",
       "      <td>(Easy, to, start, up, and, does, not, overheat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from   to    id  \\\n",
       "0            cord   neutral   41   45  2339   \n",
       "1    battery life  positive   74   86  2339   \n",
       "2  service center  negative   27   41  1316   \n",
       "3    \"sales\" team  negative  109  121  1316   \n",
       "4        tech guy   neutral    4   12  1316   \n",
       "5         quality  positive   14   21  2005   \n",
       "6             GUI  positive   36   39  2005   \n",
       "7    applications  positive  118  130  2005   \n",
       "8             use  positive  143  146  2005   \n",
       "9        start up  positive    8   16  2789   \n",
       "\n",
       "                                                text  \\\n",
       "0  I charge it at night and skip taking the cord ...   \n",
       "1  I charge it at night and skip taking the cord ...   \n",
       "2  The tech guy then said the service center does...   \n",
       "3  The tech guy then said the service center does...   \n",
       "4  The tech guy then said the service center does...   \n",
       "5  it is of high quality, has a killer GUI, is ex...   \n",
       "6  it is of high quality, has a killer GUI, is ex...   \n",
       "7  it is of high quality, has a killer GUI, is ex...   \n",
       "8  it is of high quality, has a killer GUI, is ex...   \n",
       "9  Easy to start up and does not overheat as much...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "1  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "3  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "4  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "5  [[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...   \n",
       "6  [[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...   \n",
       "7  [[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...   \n",
       "8  [[0.125, 0.25, 0.625], [0.375, 0.0, 0.625], []...   \n",
       "9  [[0.625, 0.25, 0.125], [], [], [0.0, 0.0, 1.0]...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [I, charge, night, skip, taking, cord, good, b...   \n",
       "1  [I, charge, night, skip, taking, cord, good, b...   \n",
       "2  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "3  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "4  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "5  [high, quality, ,, killer, GUI, ,, extremely, ...   \n",
       "6  [high, quality, ,, killer, GUI, ,, extremely, ...   \n",
       "7  [high, quality, ,, killer, GUI, ,, extremely, ...   \n",
       "8  [high, quality, ,, killer, GUI, ,, extremely, ...   \n",
       "9          [Easy, start, overheat, much, laptops, .]   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  \n",
       "3  (The, tech, guy, then, said, the, service, cen...  \n",
       "4  (The, tech, guy, then, said, the, service, cen...  \n",
       "5  (it, is, of, high, quality, ,, has, a, killer,...  \n",
       "6  (it, is, of, high, quality, ,, has, a, killer,...  \n",
       "7  (it, is, of, high, quality, ,, has, a, killer,...  \n",
       "8  (it, is, of, high, quality, ,, has, a, killer,...  \n",
       "9  (Easy, to, start, up, and, does, not, overheat...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ON REMPLIT LA COLONNE VIDE QU'ON A CREE \n",
    "# ---------- TRAIN DATAFRAME ---------------- #\n",
    "ps = PorterStemmer()\n",
    "i=0\n",
    "for token in df_train.text :\n",
    "    words_data= nltk.word_tokenize(str(token))\n",
    "    tokens_without_sw = [word for word in words_data if not word in stopwords.words()]\n",
    "    #print(tokens_without_sw)\n",
    "\n",
    "    pos_val = nltk.pos_tag(tokens_without_sw)\n",
    "    #print(pos_val)\n",
    "\n",
    "    scores = []\n",
    "    words_sentence = []\n",
    "\n",
    "    for (x,y) in pos_val :\n",
    "        scores.append(get_sentiment(x,y))\n",
    "        words_sentence.append(x)\n",
    "\n",
    "    df_train['Sentiword'][i] = words_sentence\n",
    "    df_train['Score_by_word'][i] = scores\n",
    "    i+=1\n",
    "\n",
    "df_train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2128</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]</td>\n",
       "      <td></td>\n",
       "      <td>[I, liked, aluminum, body, .]</td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]</td>\n",
       "      <td></td>\n",
       "      <td>[Lightweight, screen, beautiful, !]</td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>[[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...</td>\n",
       "      <td></td>\n",
       "      <td>[From, build, quality, performance, ,, everyth...</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>[[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...</td>\n",
       "      <td></td>\n",
       "      <td>[From, build, quality, performance, ,, everyth...</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>costing</td>\n",
       "      <td>positive</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>655</td>\n",
       "      <td>It was truly a great computer costing less tha...</td>\n",
       "      <td>[[], [0.625, 0.0, 0.375], [0.0, 0.0, 1.0], [0....</td>\n",
       "      <td></td>\n",
       "      <td>[It, truly, great, computer, costing, less, th...</td>\n",
       "      <td>(It, was, truly, a, great, computer, costing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Boots up</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2443</td>\n",
       "      <td>Boots up fast and runs great!</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0.0, 0...</td>\n",
       "      <td></td>\n",
       "      <td>[Boots, fast, runs, great, !]</td>\n",
       "      <td>(Boots, up, fast, and, runs, great, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>runs</td>\n",
       "      <td>positive</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>2443</td>\n",
       "      <td>Boots up fast and runs great!</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0.0, 0...</td>\n",
       "      <td></td>\n",
       "      <td>[Boots, fast, runs, great, !]</td>\n",
       "      <td>(Boots, up, fast, and, runs, great, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tech support</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>764</td>\n",
       "      <td>Call tech support, standard email the form and...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [], [0.375, 0.375, 0...</td>\n",
       "      <td></td>\n",
       "      <td>[Call, tech, support, ,, standard, email, form...</td>\n",
       "      <td>(Call, tech, support, ,, standard, email, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1479</td>\n",
       "      <td>The service I received from Toshiba went above...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [], [], [0.0...</td>\n",
       "      <td></td>\n",
       "      <td>[The, service, I, received, Toshiba, went, bey...</td>\n",
       "      <td>(The, service, I, received, from, Toshiba, wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>internet speed</td>\n",
       "      <td>positive</td>\n",
       "      <td>41</td>\n",
       "      <td>55</td>\n",
       "      <td>2937</td>\n",
       "      <td>I would recommend it just because of the inter...</td>\n",
       "      <td>[[], [], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0],...</td>\n",
       "      <td></td>\n",
       "      <td>[I, would, recommend, internet, speed, probabl...</td>\n",
       "      <td>(I, would, recommend, it, just, because, of, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from  to    id  \\\n",
       "0   aluminum body  positive   12  25  2128   \n",
       "1          screen  positive   20  26    81   \n",
       "2   build quality  negative    9  22   353   \n",
       "3     performance  negative   30  41   353   \n",
       "4         costing  positive   30  37   655   \n",
       "5        Boots up  positive    0   8  2443   \n",
       "6            runs  positive   18  22  2443   \n",
       "7    tech support   neutral    5  17   764   \n",
       "8         service  positive    4  11  1479   \n",
       "9  internet speed  positive   41  55  2937   \n",
       "\n",
       "                                                text  \\\n",
       "0                         I liked the aluminum body.   \n",
       "1           Lightweight and the screen is beautiful!   \n",
       "2  From the build quality to the performance, eve...   \n",
       "3  From the build quality to the performance, eve...   \n",
       "4  It was truly a great computer costing less tha...   \n",
       "5                      Boots up fast and runs great!   \n",
       "6                      Boots up fast and runs great!   \n",
       "7  Call tech support, standard email the form and...   \n",
       "8  The service I received from Toshiba went above...   \n",
       "9  I would recommend it just because of the inter...   \n",
       "\n",
       "                                       Score_by_word PosTag  \\\n",
       "0     [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]          \n",
       "1         [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]          \n",
       "2  [[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...          \n",
       "3  [[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...          \n",
       "4  [[], [0.625, 0.0, 0.375], [0.0, 0.0, 1.0], [0....          \n",
       "5  [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0.0, 0...          \n",
       "6  [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0.0, 0...          \n",
       "7  [[], [], [0.0, 0.0, 1.0], [], [0.375, 0.375, 0...          \n",
       "8  [[], [0.0, 0.0, 1.0], [], [], [], [], [], [0.0...          \n",
       "9  [[], [], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0],...          \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [I, liked, aluminum, body, .]   \n",
       "1                [Lightweight, screen, beautiful, !]   \n",
       "2  [From, build, quality, performance, ,, everyth...   \n",
       "3  [From, build, quality, performance, ,, everyth...   \n",
       "4  [It, truly, great, computer, costing, less, th...   \n",
       "5                      [Boots, fast, runs, great, !]   \n",
       "6                      [Boots, fast, runs, great, !]   \n",
       "7  [Call, tech, support, ,, standard, email, form...   \n",
       "8  [The, service, I, received, Toshiba, went, bey...   \n",
       "9  [I, would, recommend, internet, speed, probabl...   \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  \n",
       "3  (From, the, build, quality, to, the, performan...  \n",
       "4  (It, was, truly, a, great, computer, costing, ...  \n",
       "5             (Boots, up, fast, and, runs, great, !)  \n",
       "6             (Boots, up, fast, and, runs, great, !)  \n",
       "7  (Call, tech, support, ,, standard, email, the,...  \n",
       "8  (The, service, I, received, from, Toshiba, wen...  \n",
       "9  (I, would, recommend, it, just, because, of, t...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ON REMPLIT LA COLONNE VIDE QU'ON A CREE \n",
    "# ---------- TEST DATAFRAME ---------------- #\n",
    "ps = PorterStemmer()\n",
    "i=0\n",
    "for token in df_test.text :\n",
    "    words_data= nltk.word_tokenize(str(token))\n",
    "    tokens_without_sw = [word for word in words_data if not word in stopwords.words()]\n",
    "    #print(tokens_without_sw)\n",
    "\n",
    "    pos_val = nltk.pos_tag(tokens_without_sw)\n",
    "    #print(pos_val)\n",
    "\n",
    "    scores = []\n",
    "    words_sentence = []\n",
    "\n",
    "    for (x,y) in pos_val :\n",
    "        scores.append(get_sentiment(x,y))\n",
    "        words_sentence.append(x)\n",
    "\n",
    "    df_test['Sentiword'][i] = words_sentence\n",
    "    df_test['Score_by_word'][i] = scores\n",
    "    i+=1\n",
    "\n",
    "df_test.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ajout complémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enfin, au cas où, on ajoute une colonne avec seulement les POStag(il est plus facile de retirer des colonnes que d'en ajouter plus tard)\n",
    "\n",
    "#D'abord, On ajoute une colonne vide à nos dataframes train et test\n",
    "df_train.insert(7,'PosTag',\"\")\n",
    "df_test.insert(7,'PosTag',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On remplit train avec les pos tag\n",
    "for i in range(len(df_train.PosTag)):\n",
    "    df_train['PosTag'][i] = [(X.pos_) for X in  df_train.token_text[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On remplit test avec les pos tag\n",
    "for i in range(len(df_test.PosTag)):\n",
    "    df_test['PosTag'][i] = [(X.pos_) for X in  df_test.token_text[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On a enfin nos Dataframe propres qu'on pourra modeler à volonter (vectorizer, encoder, modifier les types etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from  to    id  \\\n",
       "0            cord   neutral   41  45  2339   \n",
       "1    battery life  positive   74  86  2339   \n",
       "2  service center  negative   27  41  1316   \n",
       "\n",
       "                                                text  \\\n",
       "0  I charge it at night and skip taking the cord ...   \n",
       "1  I charge it at night and skip taking the cord ...   \n",
       "2  The tech guy then said the service center does...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "1  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "1  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "2  [DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [I, charge, night, skip, taking, cord, good, b...   \n",
       "1  [I, charge, night, skip, taking, cord, good, b...   \n",
       "2  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>2128</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, NOUN, PUNCT]</td>\n",
       "      <td>[I, liked, aluminum, body, .]</td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]</td>\n",
       "      <td>[ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]</td>\n",
       "      <td>[Lightweight, screen, beautiful, !]</td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>353</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>[[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...</td>\n",
       "      <td>[ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...</td>\n",
       "      <td>[From, build, quality, performance, ,, everyth...</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  polarity from  to    id  \\\n",
       "0  aluminum body  positive   12  25  2128   \n",
       "1         screen  positive   20  26    81   \n",
       "2  build quality  negative    9  22   353   \n",
       "\n",
       "                                                text  \\\n",
       "0                         I liked the aluminum body.   \n",
       "1           Lightweight and the screen is beautiful!   \n",
       "2  From the build quality to the performance, eve...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0     [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]   \n",
       "1         [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]   \n",
       "2  [[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0               [PRON, VERB, DET, NOUN, NOUN, PUNCT]   \n",
       "1           [ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]   \n",
       "2  [ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [I, liked, aluminum, body, .]   \n",
       "1                [Lightweight, screen, beautiful, !]   \n",
       "2  [From, build, quality, performance, ,, everyth...   \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce que nous allons garder dans un premier temps : \n",
    "* Variables exlpicatives/prédictives :\n",
    "    * term\n",
    "    * text\n",
    "    * Score_by_word\n",
    "    * PosTag\n",
    "    * Sentiword\n",
    "* Label à prédire :\n",
    "    * polarity\n",
    "\n",
    "En effet, les colonnes \"from\", \"to\",\"id\" ne semblent pas être des éléments déterminant pour l'analyse. La colonne \"token_text\" n'est plus utile car nous avons fait une colonne de posTag et une de text, ce qui comprend les informations que nous souhaitions de token_text. L'enlever nous permet également de diminuer les dimensions de notre dataframe, car nous allons encoder ces colonnes pour plus tard la partie vectorisation (ce qui augmentent les dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>negative</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term                                               text  \\\n",
       "0            cord  I charge it at night and skip taking the cord ...   \n",
       "1    battery life  I charge it at night and skip taking the cord ...   \n",
       "2  service center  The tech guy then said the service center does...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "1  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "2  [DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [I, charge, night, skip, taking, cord, good, b...   \n",
       "1  [I, charge, night, skip, taking, cord, good, b...   \n",
       "2  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "\n",
       "                                       Score_by_word  polarity  \\\n",
       "0  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   neutral   \n",
       "1  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...  positive   \n",
       "2  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....  negative   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF_train\n",
    "DF_train = df_train[['term','text','PosTag','Sentiword','Score_by_word','polarity','token_text']]\n",
    "DF_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, NOUN, PUNCT]</td>\n",
       "      <td>[I, liked, aluminum, body, .]</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]</td>\n",
       "      <td>positive</td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td>[ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]</td>\n",
       "      <td>[Lightweight, screen, beautiful, !]</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]</td>\n",
       "      <td>positive</td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>[ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...</td>\n",
       "      <td>[From, build, quality, performance, ,, everyth...</td>\n",
       "      <td>[[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term                                               text  \\\n",
       "0  aluminum body                         I liked the aluminum body.   \n",
       "1         screen           Lightweight and the screen is beautiful!   \n",
       "2  build quality  From the build quality to the performance, eve...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0               [PRON, VERB, DET, NOUN, NOUN, PUNCT]   \n",
       "1           [ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]   \n",
       "2  [ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [I, liked, aluminum, body, .]   \n",
       "1                [Lightweight, screen, beautiful, !]   \n",
       "2  [From, build, quality, performance, ,, everyth...   \n",
       "\n",
       "                                       Score_by_word  polarity  \\\n",
       "0     [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]  positive   \n",
       "1         [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]  positive   \n",
       "2  [[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...  negative   \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF_test\n",
    "DF_test = df_test[['term','text','PosTag','Sentiword','Score_by_word','polarity','token_text']]\n",
    "DF_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder en Dataframe grace à .pkl\n",
    "#df_train.to_pickle('df_train_PANDAS_DATAFRAME_LAPTOP.pkl')  \n",
    "#df_test.to_pickle('df_test_PANDAS_DATAFRAME_LAPTOP.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from  to    id  \\\n",
       "0            cord   neutral   41  45  2339   \n",
       "1    battery life  positive   74  86  2339   \n",
       "2  service center  negative   27  41  1316   \n",
       "\n",
       "                                                text  \\\n",
       "0  I charge it at night and skip taking the cord ...   \n",
       "1  I charge it at night and skip taking the cord ...   \n",
       "2  The tech guy then said the service center does...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "1  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "1  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "2  [DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [I, charge, night, skip, taking, cord, good, b...   \n",
       "1  [I, charge, night, skip, taking, cord, good, b...   \n",
       "2  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour recharger les dataframe : \n",
    "df_train = pd.read_pickle('df_train_PANDAS_DATAFRAME_LAPTOP.pkl')\n",
    "df_test = pd.read_pickle('df_test_PANDAS_DATAFRAME_LAPTOP.pkl')\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Visualisation\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPARTITION POLARITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[460.,   0.,   0., 987.,   0.,   0., 866.,   0.,   0.,  45.],\n",
       "        [  4.,   0.,   0.,  29.,   0.,   0.,  16.,   0.,   0.,   0.]]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <a list of 2 BarContainer objects>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = DF_train['polarity']\n",
    "x2 = DF_test['polarity']\n",
    "\n",
    "colors = ['#E69F00', '#56B4E9']\n",
    "names = ['TRAIN', 'TEST']\n",
    "         \n",
    "plt.hist([x1, x2],color = colors, label=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les nombres n'étant pas très pertinent pour étudier la répartition nous regarderons plutôt le pourcentage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987 866 460 45\n",
      "29 16 4 0\n",
      "0.41857506361323155 0.3672603901611535 0.19508057675996607 0.019083969465648856\n",
      "0.5918367346938775 0.32653061224489793 0.08163265306122448 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOMBRES\n",
    "posTrain=len(DF_train[DF_train['polarity'] == 'positive' ].index)\n",
    "negTrain=len(DF_train[DF_train['polarity'] == 'negative' ].index)\n",
    "neuTrain=len(DF_train[DF_train['polarity'] == 'neutral' ].index)\n",
    "confTrain=len(DF_train[DF_train['polarity'] == 'conflict' ].index)\n",
    "\n",
    "posTest=len(DF_test[DF_test['polarity'] == 'positive' ].index)\n",
    "negTest=len(DF_test[DF_test['polarity'] == 'negative' ].index)\n",
    "neuTest=len(DF_test[DF_test['polarity'] == 'neutral' ].index)\n",
    "confTest=len(DF_test[DF_test['polarity'] == 'conflict' ].index)\n",
    "\n",
    "print(posTrain,negTrain,neuTrain,confTrain)\n",
    "print(posTest,negTest,neuTest,confTest)\n",
    "\n",
    "# TAUX\n",
    "pos_train=len(DF_train[DF_train['polarity'] == 'positive' ].index)/len(DF_train)\n",
    "neg_train=len(DF_train[DF_train['polarity'] == 'negative' ].index)/len(DF_train)\n",
    "neu_train=len(DF_train[DF_train['polarity'] == 'neutral' ].index)/len(DF_train)\n",
    "conf_train=len(DF_train[DF_train['polarity'] == 'conflict' ].index)/len(DF_train)\n",
    "\n",
    "pos_test=len(DF_test[DF_test['polarity'] == 'positive' ].index)/len(DF_test)\n",
    "neg_test=len(DF_test[DF_test['polarity'] == 'negative' ].index)/len(DF_test)\n",
    "neu_test=len(DF_test[DF_test['polarity'] == 'neutral' ].index)/len(DF_test)\n",
    "conf_test=len(DF_test[DF_test['polarity'] == 'conflict' ].index)/len(DF_test)\n",
    "\n",
    "print(pos_train,neg_train,neu_train,conf_train)\n",
    "print(pos_test,neg_test,neu_test,conf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x211ced04640>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "barWidth = 0.4\n",
    "train = [pos_train, neg_train, neu_train, conf_train]\n",
    "test = [pos_test, neg_test, neu_test, conf_test]\n",
    "r1 = range(len(train))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "plotTrain= plt.bar(r1, train, width = barWidth, color = ['red' for i in train])\n",
    "plotTest= plt.bar(r2, test, width = barWidth, color = ['blue' for i in train])\n",
    "plt.xticks([r + barWidth / 2 for r in range(len(train))], ['positive', 'negative', 'neutral','conflict'])\n",
    "plt.legend([plotTrain, plotTest], ['TRAIN', 'TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\"> Il n'y a pas de \"conflict\" dans l'ensemble de test, on ne prendra pas en compte ces valeurs. <font>**\n",
    "\n",
    "**<font color=\"green\">Les autres classes semblent bien réparties.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_train[DF_train['polarity'] == 'conflict' ].index\n",
    "# on retire les lignes concernées\n",
    "DF_train.drop(indexNames , inplace=True)\n",
    "\n",
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_test[DF_test['polarity'] == 'conflict' ].index\n",
    "# on retire les lignes concernées\n",
    "DF_test.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_train[DF_train['polarity'] == 'conflict' ].index\n",
    "\n",
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_test[DF_test['polarity'] == 'conflict' ].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vérification rapide**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>74</td>\n",
       "      <td>86</td>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...</td>\n",
       "      <td>[I, charge, night, skip, taking, cord, good, b...</td>\n",
       "      <td>(I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....</td>\n",
       "      <td>[DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...</td>\n",
       "      <td>[The, tech, guy, said, service, center, 1-to-1...</td>\n",
       "      <td>(The, tech, guy, then, said, the, service, cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from  to    id  \\\n",
       "0            cord   neutral   41  45  2339   \n",
       "1    battery life  positive   74  86  2339   \n",
       "2  service center  negative   27  41  1316   \n",
       "\n",
       "                                                text  \\\n",
       "0  I charge it at night and skip taking the cord ...   \n",
       "1  I charge it at night and skip taking the cord ...   \n",
       "2  The tech guy then said the service center does...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "1  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [0....   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "1  [PRON, VERB, PRON, ADP, NOUN, CCONJ, NOUN, VER...   \n",
       "2  [DET, NOUN, NOUN, ADV, VERB, DET, NOUN, NOUN, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [I, charge, night, skip, taking, cord, good, b...   \n",
       "1  [I, charge, night, skip, taking, cord, good, b...   \n",
       "2  [The, tech, guy, said, service, center, 1-to-1...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "1  (I, charge, it, at, night, and, skip, taking, ...  \n",
       "2  (The, tech, guy, then, said, the, service, cen...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(DF_test[DF_test['polarity'] == 'conflict' ].index))\n",
    "print(len(DF_test[DF_test['polarity'] == 'conflict' ].index))\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder en Dataframe grace à .pkl\n",
    "#DF_train.to_pickle('DF_train_PANDAS_DATAFRAME_LAPTOP.pkl')  \n",
    "#DF_test.to_pickle('DF_test_PANDAS_DATAFRAME_LAPTOP.pkl')  \n",
    "#Pour recharger les dataframe : \n",
    "DF_train = pd.read_pickle('DF_train_PANDAS_DATAFRAME_LAPTOP.pkl')\n",
    "DF_test = pd.read_pickle('DF_test_PANDAS_DATAFRAME_LAPTOP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>gaming</td>\n",
       "      <td>However, the multi-touch gestures and large tr...</td>\n",
       "      <td>[ADV, PUNCT, DET, ADJ, ADJ, ADJ, NOUN, CCONJ, ...</td>\n",
       "      <td>[However, ,, multi-touch, gestures, large, tra...</td>\n",
       "      <td>[[0.125, 0.5, 0.375], [], [], [0.0, 0.0, 1.0],...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>(However, ,, the, multi, -, touch, gestures, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>suite of software</td>\n",
       "      <td>I love the way the entire suite of software wo...</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, DET, ADJ, NOUN, ADP, N...</td>\n",
       "      <td>[I, love, way, entire, suite, software, works,...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0....</td>\n",
       "      <td>positive</td>\n",
       "      <td>(I, love, the, way, the, entire, suite, of, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>speed</td>\n",
       "      <td>The speed is incredible and I am more than sat...</td>\n",
       "      <td>[DET, NOUN, AUX, ADJ, CCONJ, PRON, AUX, ADJ, S...</td>\n",
       "      <td>[The, speed, incredible, I, satisfied, .]</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(The, speed, is, incredible, and, I, am, more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Windows 7</td>\n",
       "      <td>This laptop meets every expectation and Window...</td>\n",
       "      <td>[DET, NOUN, VERB, DET, NOUN, CCONJ, PROPN, NUM...</td>\n",
       "      <td>[This, laptop, meets, every, expectation, Wind...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [], [0.0, 0.0, 1.0],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(This, laptop, meets, every, expectation, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>usb devices</td>\n",
       "      <td>I can barely use any usb devices because they ...</td>\n",
       "      <td>[PRON, VERB, ADV, VERB, DET, NOUN, NOUN, SCONJ...</td>\n",
       "      <td>[I, barely, use, usb, devices, stay, connected...</td>\n",
       "      <td>[[], [0.125, 0.0, 0.875], [], [], [0.0, 0.0, 1...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(I, can, barely, use, any, usb, devices, becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>software</td>\n",
       "      <td>When I finally had everything running with all...</td>\n",
       "      <td>[ADV, PRON, ADV, AUX, PRON, VERB, ADP, DET, DE...</td>\n",
       "      <td>[When, I, finally, everything, running, softwa...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [], [], [0.0, 0.0, 1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>(When, I, finally, had, everything, running, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>system</td>\n",
       "      <td>When I finally had everything running with all...</td>\n",
       "      <td>[ADV, PRON, ADV, AUX, PRON, VERB, ADP, DET, DE...</td>\n",
       "      <td>[When, I, finally, everything, running, softwa...</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [], [], [0.0, 0.0, 1...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(When, I, finally, had, everything, running, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Microsoft office for the mac</td>\n",
       "      <td>One suggestion I do have, is to not bother get...</td>\n",
       "      <td>[NUM, NOUN, PRON, AUX, AUX, PUNCT, AUX, PART, ...</td>\n",
       "      <td>[One, suggestion, I, ,, bother, getting, Micro...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [], [], [0.0...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(One, suggestion, I, do, have, ,, is, to, not,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                          term  \\\n",
       "25     25                        gaming   \n",
       "26     26             suite of software   \n",
       "27     27                         speed   \n",
       "28     28                     Windows 7   \n",
       "29     29                   usb devices   \n",
       "30     31                      software   \n",
       "31     32                        system   \n",
       "32     33  Microsoft office for the mac   \n",
       "\n",
       "                                                 text  \\\n",
       "25  However, the multi-touch gestures and large tr...   \n",
       "26  I love the way the entire suite of software wo...   \n",
       "27  The speed is incredible and I am more than sat...   \n",
       "28  This laptop meets every expectation and Window...   \n",
       "29  I can barely use any usb devices because they ...   \n",
       "30  When I finally had everything running with all...   \n",
       "31  When I finally had everything running with all...   \n",
       "32  One suggestion I do have, is to not bother get...   \n",
       "\n",
       "                                               PosTag  \\\n",
       "25  [ADV, PUNCT, DET, ADJ, ADJ, ADJ, NOUN, CCONJ, ...   \n",
       "26  [PRON, VERB, DET, NOUN, DET, ADJ, NOUN, ADP, N...   \n",
       "27  [DET, NOUN, AUX, ADJ, CCONJ, PRON, AUX, ADJ, S...   \n",
       "28  [DET, NOUN, VERB, DET, NOUN, CCONJ, PROPN, NUM...   \n",
       "29  [PRON, VERB, ADV, VERB, DET, NOUN, NOUN, SCONJ...   \n",
       "30  [ADV, PRON, ADV, AUX, PRON, VERB, ADP, DET, DE...   \n",
       "31  [ADV, PRON, ADV, AUX, PRON, VERB, ADP, DET, DE...   \n",
       "32  [NUM, NOUN, PRON, AUX, AUX, PUNCT, AUX, PART, ...   \n",
       "\n",
       "                                            Sentiword  \\\n",
       "25  [However, ,, multi-touch, gestures, large, tra...   \n",
       "26  [I, love, way, entire, suite, software, works,...   \n",
       "27          [The, speed, incredible, I, satisfied, .]   \n",
       "28  [This, laptop, meets, every, expectation, Wind...   \n",
       "29  [I, barely, use, usb, devices, stay, connected...   \n",
       "30  [When, I, finally, everything, running, softwa...   \n",
       "31  [When, I, finally, everything, running, softwa...   \n",
       "32  [One, suggestion, I, ,, bother, getting, Micro...   \n",
       "\n",
       "                                        Score_by_word  polarity  \\\n",
       "25  [[0.125, 0.5, 0.375], [], [], [0.0, 0.0, 1.0],...   neutral   \n",
       "26  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0....  positive   \n",
       "27  [[], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], [],...  positive   \n",
       "28  [[], [], [0.0, 0.0, 1.0], [], [0.0, 0.0, 1.0],...  positive   \n",
       "29  [[], [0.125, 0.0, 0.875], [], [], [0.0, 0.0, 1...  negative   \n",
       "30  [[], [], [0.0, 0.0, 1.0], [], [], [0.0, 0.0, 1...   neutral   \n",
       "31  [[], [], [0.0, 0.0, 1.0], [], [], [0.0, 0.0, 1...  negative   \n",
       "32  [[], [0.0, 0.0, 1.0], [], [], [], [], [], [0.0...  negative   \n",
       "\n",
       "                                           token_text  \n",
       "25  (However, ,, the, multi, -, touch, gestures, a...  \n",
       "26  (I, love, the, way, the, entire, suite, of, so...  \n",
       "27  (The, speed, is, incredible, and, I, am, more,...  \n",
       "28  (This, laptop, meets, every, expectation, and,...  \n",
       "29  (I, can, barely, use, any, usb, devices, becau...  \n",
       "30  (When, I, finally, had, everything, running, w...  \n",
       "31  (When, I, finally, had, everything, running, w...  \n",
       "32  (One, suggestion, I, do, have, ,, is, to, not,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train.reset_index(inplace=True, drop=False)\n",
    "DF_train[25:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aluminum body</td>\n",
       "      <td>I liked the aluminum body.</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, NOUN, PUNCT]</td>\n",
       "      <td>[I, liked, aluminum, body, .]</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]</td>\n",
       "      <td>positive</td>\n",
       "      <td>(I, liked, the, aluminum, body, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>Lightweight and the screen is beautiful!</td>\n",
       "      <td>[ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]</td>\n",
       "      <td>[Lightweight, screen, beautiful, !]</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]</td>\n",
       "      <td>positive</td>\n",
       "      <td>(Lightweight, and, the, screen, is, beautiful, !)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build quality</td>\n",
       "      <td>From the build quality to the performance, eve...</td>\n",
       "      <td>[ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...</td>\n",
       "      <td>[From, build, quality, performance, ,, everyth...</td>\n",
       "      <td>[[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(From, the, build, quality, to, the, performan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term                                               text  \\\n",
       "0  aluminum body                         I liked the aluminum body.   \n",
       "1         screen           Lightweight and the screen is beautiful!   \n",
       "2  build quality  From the build quality to the performance, eve...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0               [PRON, VERB, DET, NOUN, NOUN, PUNCT]   \n",
       "1           [ADV, CCONJ, DET, NOUN, AUX, ADJ, PUNCT]   \n",
       "2  [ADP, DET, NOUN, NOUN, ADP, DET, NOUN, PUNCT, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [I, liked, aluminum, body, .]   \n",
       "1                [Lightweight, screen, beautiful, !]   \n",
       "2  [From, build, quality, performance, ,, everyth...   \n",
       "\n",
       "                                       Score_by_word  polarity  \\\n",
       "0     [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]  positive   \n",
       "1         [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [], []]  positive   \n",
       "2  [[], [], [0.375, 0.0, 0.625], [0.125, 0.0, 0.8...  negative   \n",
       "\n",
       "                                          token_text  \n",
       "0                 (I, liked, the, aluminum, body, .)  \n",
       "1  (Lightweight, and, the, screen, is, beautiful, !)  \n",
       "2  (From, the, build, quality, to, the, performan...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_test.reset_index(inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cord'\n",
      "  'I charge it at night and skip taking the cord with me because of the good battery life.'\n",
      "  'PRON VERB PRON ADP NOUN CCONJ NOUN VERB DET NOUN ADP PRON SCONJ ADP DET PROPN PROPN NOUN PUNCT'\n",
      "  'I charge night skip taking cord good battery life .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['battery life'\n",
      "  'I charge it at night and skip taking the cord with me because of the good battery life.'\n",
      "  'PRON VERB PRON ADP NOUN CCONJ NOUN VERB DET NOUN ADP PRON SCONJ ADP DET PROPN PROPN NOUN PUNCT'\n",
      "  'I charge night skip taking cord good battery life .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['service center'\n",
      "  'The tech guy then said the service center does not do 1-to-1 exchange and I have to direct my concern to the \"sales\" team, which is the retail shop which I bought my netbook from.'\n",
      "  'DET NOUN NOUN ADV VERB DET NOUN NOUN AUX PART AUX NUM NOUN CCONJ PRON AUX PART VERB DET NOUN ADP DET PUNCT NOUN PUNCT NOUN PUNCT DET AUX DET ADJ NOUN DET PRON VERB DET NOUN ADP PUNCT'\n",
      "  \"The tech guy said service center 1-to-1 exchange I direct concern `` sales '' team , retail shop I bought netbook .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[],[]']\n",
      " ...\n",
      " ['Windows Server 2008 Enterprise'\n",
      "  'We also use Paralles so we can run virtual machines of Windows XP Professional, Windows 7 Home Premium, Windows Server Enterprise 2003, and Windows Server 2008 Enterprise.'\n",
      "  'PRON ADV VERB PROPN SCONJ PRON VERB VERB ADJ NOUN ADP PROPN PROPN PROPN PUNCT PROPN NUM PROPN PROPN PUNCT PROPN PROPN PROPN NUM PUNCT CCONJ PROPN PROPN NUM PROPN PUNCT'\n",
      "  'We use Paralles run virtual machines Windows XP Professional , Windows 7 Home Premium , Windows Server Enterprise 2003 , Windows Server 2008 Enterprise .'\n",
      "  '[],[],[],[],[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.375, 0.25, 0.375],[],[],[],[0.0, 0.0, 1.0],[],[0.375, 0.25, 0.375],[]']\n",
      " ['repair'\n",
      "  'How Toshiba handles the repair seems to vary, some folks  indicate that they were charged for even an intial fix, others had the  repair done 5 times.'\n",
      "  'ADV PROPN VERB DET NOUN VERB PART VERB PUNCT DET NOUN SPACE VERB SCONJ PRON AUX VERB ADP ADV DET ADJ NOUN PUNCT NOUN AUX DET SPACE NOUN VERB NUM NOUN PUNCT'\n",
      "  'How Toshiba handles repair seems vary , folks indicate charged even intial fix , others repair done 5 times .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[],[],[0.125, 0.0, 0.875],[],[0.0, 0.5, 0.5],[],[],[],[],[],[0.5, 0.0, 0.5],[]']\n",
      " ['operating system'\n",
      "  'I would like to use a different operating system altogether.'\n",
      "  'PRON VERB VERB PART VERB DET ADJ NOUN NOUN ADV PUNCT'\n",
      "  'I would like use different operating system altogether .'\n",
      "  '[],[],[],[],[0.625, 0.0, 0.375],[],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[]']]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DATASET\n",
    "df_train = DF_train\n",
    "\n",
    "list_train = []\n",
    "label_train = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    pos_tag = [X.pos_ for X in df_train.token_text[i]]\n",
    "    pos_tag_text = \" \".join(pos_tag)\n",
    "\n",
    "    senti_word = \" \".join(df_train.Sentiword[i])\n",
    "    score_by_word = \",\".join(str(v) for v in df_train.Score_by_word[i])\n",
    "    \n",
    "    list_train.append(\n",
    "        (df_train.term[i],\n",
    "        df_train.text[i],\n",
    "        pos_tag_text,\n",
    "        senti_word,\n",
    "        score_by_word,\n",
    "        ))\n",
    "    label_train.append(df_train.polarity[i])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "LAPTOP_Train = np.array(list_train)\n",
    "print(LAPTOP_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aluminum body' 'I liked the aluminum body.'\n",
      "  'PRON VERB DET NOUN NOUN PUNCT' 'I liked aluminum body .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['screen' 'Lightweight and the screen is beautiful!'\n",
      "  'ADV CCONJ DET NOUN AUX ADJ PUNCT' 'Lightweight screen beautiful !'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[]']\n",
      " ['build quality'\n",
      "  'From the build quality to the performance, everything about it has been sub-par from what I would have expected from Apple.'\n",
      "  'ADP DET NOUN NOUN ADP DET NOUN PUNCT PRON ADP PRON AUX AUX ADJ ADJ ADJ ADP PRON PRON VERB AUX VERB ADP PROPN PUNCT'\n",
      "  'From build quality performance , everything sub-par I would expected Apple .'\n",
      "  '[],[],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['performance'\n",
      "  'From the build quality to the performance, everything about it has been sub-par from what I would have expected from Apple.'\n",
      "  'ADP DET NOUN NOUN ADP DET NOUN PUNCT PRON ADP PRON AUX AUX ADJ ADJ ADJ ADP PRON PRON VERB AUX VERB ADP PROPN PUNCT'\n",
      "  'From build quality performance , everything sub-par I would expected Apple .'\n",
      "  '[],[],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['costing'\n",
      "  'It was truly a great computer costing less than one thousand bucks before tax.'\n",
      "  'PRON AUX ADV DET ADJ NOUN VERB ADJ SCONJ NUM NUM NOUN ADP NOUN PUNCT'\n",
      "  'It truly great computer costing less thousand bucks tax .'\n",
      "  '[],[0.625, 0.0, 0.375],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.5, 0.5],[0.0, 0.0, 1.0],[0.0, 0.5, 0.5],[0.0, 0.0, 1.0],[]']\n",
      " ['Boots up' 'Boots up fast and runs great!'\n",
      "  'NOUN ADP ADV CCONJ VERB ADJ PUNCT' 'Boots fast runs great !'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['runs' 'Boots up fast and runs great!'\n",
      "  'NOUN ADP ADV CCONJ VERB ADJ PUNCT' 'Boots fast runs great !'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['tech support'\n",
      "  'Call tech support, standard email the form and fax it back in to us.'\n",
      "  'VERB NOUN NOUN PUNCT ADJ NOUN DET NOUN CCONJ VERB PRON ADV ADV ADP PRON PUNCT'\n",
      "  'Call tech support , standard email form fax back us .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[],[0.375, 0.375, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[]']\n",
      " ['service'\n",
      "  'The service I received from Toshiba went above and beyond the call of duty.'\n",
      "  'DET NOUN PRON VERB ADP PROPN VERB ADV CCONJ ADP DET NOUN ADP NOUN PUNCT'\n",
      "  'The service I received Toshiba went beyond call duty .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['internet speed'\n",
      "  'I would recommend it just because of the internet speed probably because thats the only thing i really care about.'\n",
      "  'PRON VERB VERB PRON ADV SCONJ ADP DET NOUN NOUN ADV SCONJ DET VERB DET ADJ NOUN PRON ADV VERB ADP PUNCT'\n",
      "  'I would recommend internet speed probably thats thing really .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[]']\n",
      " ['screen' 'The screen shows great colors.'\n",
      "  'DET NOUN VERB ADJ NOUN PUNCT' 'The screen shows great colors .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['battery life'\n",
      "  \"The battery life has not decreased since I bought it, so i'm thrilled with that.\"\n",
      "  'DET NOUN NOUN AUX PART VERB SCONJ PRON VERB PRON PUNCT ADV PRON AUX VERB ADP DET PUNCT'\n",
      "  \"The battery life decreased since I bought , 'm thrilled .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[],[],[],[],[],[]']\n",
      " ['price' 'The price and features more than met my needs.'\n",
      "  'DET NOUN CCONJ VERB ADV SCONJ VERB DET NOUN PUNCT'\n",
      "  'The price features needs .' '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[]']\n",
      " ['features' 'The price and features more than met my needs.'\n",
      "  'DET NOUN CCONJ VERB ADV SCONJ VERB DET NOUN PUNCT'\n",
      "  'The price features needs .' '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[]']\n",
      " ['mouse buttons' 'the mouse buttons are hard to push.'\n",
      "  'DET NOUN NOUN AUX ADJ PART VERB PUNCT' 'mouse buttons hard push .'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.75, 0.25],[0.0, 0.0, 1.0],[]']\n",
      " ['design' 'This is the complete opposite to an ergonomic design.'\n",
      "  'DET AUX DET ADJ NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'This complete opposite ergonomic design .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.125, 0.875],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['technical service'\n",
      "  'The technical service for dell is so 3rd world it might as well not even bother.'\n",
      "  'DET ADJ NOUN ADP PROPN AUX ADV ADJ NOUN PRON VERB ADV ADV PART ADV NOUN PUNCT'\n",
      "  'The technical service 3rd world might well even bother .'\n",
      "  '[],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[],[]']\n",
      " ['built in camera'\n",
      "  'The built in camera is very useful when chatting with other techs in remote buildings on our campus.'\n",
      "  'DET VERB ADP NOUN AUX ADV ADJ ADV VERB ADP ADJ NOUN ADP ADJ NOUN ADP DET NOUN PUNCT'\n",
      "  'The built camera useful chatting techs remote buildings campus .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['Keyboard' 'Keyboard good sized and wasy to use.'\n",
      "  'VERB ADJ ADJ CCONJ ADJ PART VERB PUNCT'\n",
      "  'Keyboard good sized wasy use .'\n",
      "  '[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['wifi' 'Great wifi too.' 'ADJ VERB ADV PUNCT' 'Great wifi .'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['Games' 'Games being the main issue.' 'NOUN AUX DET ADJ NOUN PUNCT'\n",
      "  'Games main issue .'\n",
      "  '[0.0, 0.0, 1.0],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[]']\n",
      " ['price'\n",
      "  'The price is another driving influence that made me purchase this laptop.'\n",
      "  'DET NOUN AUX DET NOUN NOUN DET VERB PRON VERB DET NOUN PUNCT'\n",
      "  'The price another driving influence made purchase laptop .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['price tag'\n",
      "  'But see the macbook pro is different because it may have a huge price tag but it comes with the full software that you would actually need and most of it has free future updates.'\n",
      "  'CCONJ VERB DET NOUN ADJ AUX ADJ SCONJ PRON VERB AUX DET ADJ NOUN NOUN CCONJ PRON VERB ADP DET ADJ NOUN DET PRON VERB ADV VERB CCONJ ADJ ADP PRON AUX ADJ ADJ NOUN PUNCT'\n",
      "  'But see macbook pro different may huge price tag comes full software would actually need free future updates .'\n",
      "  '[],[],[],[],[0.625, 0.0, 0.375],[],[0.0, 0.125, 0.875],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.375, 0.0, 0.625],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['software'\n",
      "  'But see the macbook pro is different because it may have a huge price tag but it comes with the full software that you would actually need and most of it has free future updates.'\n",
      "  'CCONJ VERB DET NOUN ADJ AUX ADJ SCONJ PRON VERB AUX DET ADJ NOUN NOUN CCONJ PRON VERB ADP DET ADJ NOUN DET PRON VERB ADV VERB CCONJ ADJ ADP PRON AUX ADJ ADJ NOUN PUNCT'\n",
      "  'But see macbook pro different may huge price tag comes full software would actually need free future updates .'\n",
      "  '[],[],[],[],[0.625, 0.0, 0.375],[],[0.0, 0.125, 0.875],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.375, 0.0, 0.625],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['updates'\n",
      "  'But see the macbook pro is different because it may have a huge price tag but it comes with the full software that you would actually need and most of it has free future updates.'\n",
      "  'CCONJ VERB DET NOUN ADJ AUX ADJ SCONJ PRON VERB AUX DET ADJ NOUN NOUN CCONJ PRON VERB ADP DET ADJ NOUN DET PRON VERB ADV VERB CCONJ ADJ ADP PRON AUX ADJ ADJ NOUN PUNCT'\n",
      "  'But see macbook pro different may huge price tag comes full software would actually need free future updates .'\n",
      "  '[],[],[],[],[0.625, 0.0, 0.375],[],[0.0, 0.125, 0.875],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.375, 0.0, 0.625],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['price' \"It's a great product for a great price!\"\n",
      "  'PRON AUX DET ADJ NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  \"It 's great product great price !\"\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['speed' 'Excellent speed for processing data.'\n",
      "  'ADJ NOUN ADP NOUN NOUN PUNCT' 'Excellent speed processing data .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.0, 0.0, 1.0],[]']\n",
      " ['twin packing'\n",
      "  'The Macbook arrived in a nice twin packing and sealed in the box, all the functions works great.'\n",
      "  'DET PROPN VERB ADP DET ADJ NOUN NOUN CCONJ VERB ADP DET NOUN PUNCT DET DET NOUN VERB ADJ PUNCT'\n",
      "  'The Macbook arrived nice twin packing sealed box , functions works great .'\n",
      "  '[],[],[],[0.875, 0.0, 0.125],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['functions'\n",
      "  'The Macbook arrived in a nice twin packing and sealed in the box, all the functions works great.'\n",
      "  'DET PROPN VERB ADP DET ADJ NOUN NOUN CCONJ VERB ADP DET NOUN PUNCT DET DET NOUN VERB ADJ PUNCT'\n",
      "  'The Macbook arrived nice twin packing sealed box , functions works great .'\n",
      "  '[],[],[],[0.875, 0.0, 0.125],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['switchable graphic card'\n",
      "  'The switchable graphic card is pretty sweet when you want gaming on the laptop.'\n",
      "  'DET ADJ ADJ NOUN AUX ADV ADJ ADV PRON VERB VERB ADP DET NOUN PUNCT'\n",
      "  'The switchable graphic card pretty sweet gaming laptop .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.25, 0.625],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['gaming'\n",
      "  'The switchable graphic card is pretty sweet when you want gaming on the laptop.'\n",
      "  'DET ADJ ADJ NOUN AUX ADV ADJ ADV PRON VERB VERB ADP DET NOUN PUNCT'\n",
      "  'The switchable graphic card pretty sweet gaming laptop .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.25, 0.625],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['battery life' 'I would like at least a 4 hr. battery life.'\n",
      "  'PRON VERB VERB ADP ADJ DET NUM NOUN PUNCT NOUN NOUN PUNCT'\n",
      "  'I would like least 4 hr . battery life .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['speed' 'It has good speed and plenty of hard drive space.'\n",
      "  'PRON AUX ADJ NOUN CCONJ NOUN ADP ADJ NOUN NOUN PUNCT'\n",
      "  'It good speed plenty hard drive space .'\n",
      "  '[],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.75, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['hard drive space' 'It has good speed and plenty of hard drive space.'\n",
      "  'PRON AUX ADJ NOUN CCONJ NOUN ADP ADJ NOUN NOUN PUNCT'\n",
      "  'It good speed plenty hard drive space .'\n",
      "  '[],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.75, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['driver updates'\n",
      "  \"The driver updates don't fix the issue, very frustrating.\"\n",
      "  'DET NOUN NOUN AUX PART VERB DET NOUN PUNCT ADV ADJ PUNCT'\n",
      "  \"The driver updates n't fix issue , frustrating .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[],[],[0.125, 0.0, 0.875],[],[],[]']\n",
      " ['features' 'Im glad that it has such great features in it.'\n",
      "  'PRON VERB ADJ SCONJ PRON AUX ADJ ADJ NOUN ADP PRON PUNCT'\n",
      "  'Im glad great features .' '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['performance'\n",
      "  'I always have used a tower home PC and jumped to the laptop and have been very satisfied with its performance.'\n",
      "  'PRON ADV AUX VERB DET NOUN NOUN NOUN CCONJ VERB ADP DET NOUN CCONJ AUX AUX ADV ADJ ADP DET NOUN PUNCT'\n",
      "  'I always used tower home PC jumped laptop satisfied performance .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[]']\n",
      " ['GarageBand'\n",
      "  'My sister has the same Mac as me and she is in a band and uses GarageBand to record and edit.'\n",
      "  'DET NOUN AUX DET ADJ PROPN SCONJ PRON CCONJ PRON AUX ADP DET NOUN CCONJ VERB PROPN PART VERB CCONJ NOUN PUNCT'\n",
      "  'My sister Mac band uses GarageBand record edit .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['price'\n",
      "  'and looks very sexyy:D really the mac book pro is the best laptop specially for students in college if you are not caring about price.'\n",
      "  'CCONJ VERB ADV NOUN PUNCT PROPN ADV DET PROPN NOUN PROPN AUX DET ADJ NOUN ADV ADP NOUN ADP NOUN SCONJ PRON AUX PART VERB ADP NOUN PUNCT'\n",
      "  'looks sexyy : D really mac book pro best laptop specially students college caring price .'\n",
      "  '[0.0, 0.0, 1.0],[],[],[0.125, 0.25, 0.625],[0.625, 0.0, 0.375],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['extended warranty' 'Also, the extended warranty was a problem.'\n",
      "  'ADV PUNCT DET ADJ NOUN AUX DET NOUN PUNCT'\n",
      "  'Also , extended warranty problem .'\n",
      "  '[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.0, 0.625, 0.375],[]']\n",
      " ['support'\n",
      "  'My opinion of Sony has been dropping as fast as the stock market, given their horrible support, but this machine just caused another plunge.'\n",
      "  'DET NOUN ADP PROPN AUX AUX VERB ADV ADV SCONJ DET NOUN NOUN PUNCT VERB DET ADJ NOUN PUNCT CCONJ DET NOUN ADV VERB DET NOUN PUNCT'\n",
      "  'My opinion Sony dropping fast stock market , given horrible support , machine caused another plunge .'\n",
      "  '[],[0.0, 0.625, 0.375],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.625, 0.375],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['glass screen' 'I also liked the glass screen.'\n",
      "  'PRON ADV VERB DET NOUN NOUN PUNCT' 'I liked glass screen .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['technical support'\n",
      "  \"When I called Toshiba, they would not do anything and even tried to charge me $35 for the phone call, even though they didn't offer any technical support.\"\n",
      "  'ADV PRON VERB PROPN PUNCT PRON VERB PART AUX PRON CCONJ ADV VERB PART VERB PRON SYM NUM ADP DET NOUN NOUN PUNCT ADV SCONJ PRON AUX PART VERB DET ADJ NOUN PUNCT'\n",
      "  \"When I called Toshiba , would anything even tried charge $ 35 phone call , even though n't offer technical support .\"\n",
      "  '[],[],[],[],[],[],[],[0.125, 0.0, 0.875],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[],[],[],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[]']\n",
      " ['staff'\n",
      "  '(Beware, their staff could send you back making you feel that only they know what a computer is.'\n",
      "  'PUNCT VERB PUNCT DET NOUN VERB VERB PRON ADV VERB PRON VERB SCONJ ADV PRON VERB PRON DET NOUN AUX PUNCT'\n",
      "  '( Beware , staff could send back making feel know computer .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[],[0.0, 0.0, 1.0],[]']\n",
      " ['programs'\n",
      "  \"The only thing that I don't like about my mac is that sometimes there are programs that I want to be able to run and I am not able to.\"\n",
      "  'DET ADJ NOUN DET PRON AUX PART VERB ADP DET NOUN AUX SCONJ ADV PRON AUX NOUN DET PRON VERB PART AUX ADJ PART VERB CCONJ PRON AUX PART ADJ PART PUNCT'\n",
      "  \"The thing I n't like mac sometimes programs I able run I able .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[]']\n",
      " ['Wireless'\n",
      "  'Wireless has not been a issue for me, like some others have meantioned.'\n",
      "  'NOUN AUX PART AUX DET NOUN ADP PRON PUNCT SCONJ DET NOUN AUX VERB PUNCT'\n",
      "  'Wireless issue , like others meantioned .'\n",
      "  '[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[],[],[],[],[]']\n",
      " ['battery life'\n",
      "  \"MacBook Notebooks quickly die out because of their short battery life, as well as the many background programs that run without the user's knowlede.\"\n",
      "  'PROPN PROPN ADV VERB ADP SCONJ ADP DET ADJ NOUN NOUN PUNCT ADV ADV SCONJ DET ADJ NOUN NOUN DET VERB ADP DET NOUN PART NOUN PUNCT'\n",
      "  \"MacBook Notebooks quickly short battery life , well many background programs run without user 's knowlede .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[]']\n",
      " ['background programs'\n",
      "  \"MacBook Notebooks quickly die out because of their short battery life, as well as the many background programs that run without the user's knowlede.\"\n",
      "  'PROPN PROPN ADV VERB ADP SCONJ ADP DET ADJ NOUN NOUN PUNCT ADV ADV SCONJ DET ADJ NOUN NOUN DET VERB ADP DET NOUN PART NOUN PUNCT'\n",
      "  \"MacBook Notebooks quickly short battery life , well many background programs run without user 's knowlede .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[]']\n",
      " ['price' 'All for such a great price.' 'DET ADP DET DET ADJ NOUN PUNCT'\n",
      "  'All great price .' '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']]\n"
     ]
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "df_test = DF_test\n",
    "\n",
    "list_test = []\n",
    "label_test = []\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    pos_tag = [X.pos_ for X in  df_test.token_text[i]]\n",
    "    pos_tag_text = \" \".join(pos_tag)\n",
    "    senti_word = \" \".join(df_test.Sentiword[i])\n",
    "    score_by_word = \",\".join(str(v) for v in df_test.Score_by_word[i])\n",
    "    \n",
    "    list_test.append(\n",
    "        (df_test.term[i],\n",
    "        df_test.text[i],\n",
    "        pos_tag_text,\n",
    "        senti_word,\n",
    "        score_by_word,\n",
    "        ))\n",
    "    label_test.append(df_test.polarity[i])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "LAPTOP_Test = np.array(list_test)\n",
    "print(LAPTOP_Test)\n",
    "#print(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation — TF/IDF\n",
    "\n",
    "* Aux fins de la plupart des modélisations mathématiques effectuées sur le texte et aux fins de cette expérimentation, différents processus de « vectorisation » ont été mis en oeuvre.\n",
    "\n",
    "* Le contenu textuel seul ne peut pas être modifié et contraint dans l'espace mathématique sans être transformé en nombres dans le but d'être lu par un algorithme d'apprentissage automatique.\n",
    "\n",
    "* C'est pourquoi pour les besoins des méthodes supervisées dans ce projet, différents types de vectorisation ont été utilisés pour convertir des données qualitatives en données quantitatives afin de les manipuler mathématiquement. Ces vecteurs deviennent des caractéristiques intégrées pour les modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fréquence de terme/Fréquence de document inverse (TF/IDF)**\n",
    "\n",
    "Il s'agit de la technique de vectorisation utilisée pour le modèle Support Vector Machine. \n",
    "* TF/IDF a été déployé sur les données d'entraînement avec une approche unigramme qui compte chaque mot individuel comme un terme. La « fréquence des termes » correspond à la fréquence à laquelle un certain mot apparaît dans le texte, la « fréquence inverse du document » fait référence à la réduction de la signification des mots qui apparaissent le plus souvent dans tout le texte.\n",
    "* Cela sert à faire des mots que l'on voit fréquemment dans un document donné mais pas nécessairement dans tous les documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier test avec seulement la colonne text comme variable explicative\n",
    "\n",
    "Ce premier test ne signifie pas grand chose, mais il permet de montrer l'efficacité de SVM. Il doit prédire sans savoir le term à analyser, et donc sans savoir pourquoi la même phrase peut avoir une polarité positive et parfois négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.540410s; Prediction time: 0.009003s\n",
      "positive:  {'precision': 0.8620689655172413, 'recall': 0.8620689655172413, 'f1-score': 0.8620689655172413, 'support': 29}\n",
      "negative:  {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 16}\n",
      "neutral:  {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(DF_train['text'])\n",
    "test_vectors = vectorizer.transform(DF_test['text'])\n",
    "\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, DF_train['polarity'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(DF_test['polarity'], prediction_linear, output_dict=True)\n",
    "\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont très intéressants malgré qu'ils ne soient pas interprétable. \n",
    "<font color='red'> Cependant ce n'est pas ce que nous cherchons. Là, notre modèle prédit positif ou négatif sans savoir le term dont il s'agit. Il n'a que les phrases pour prédire. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On doit d'abord normaliser les dimensions de nos listes. De sorte que la liste ait toujours le même nombre de features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aluminum body I liked the aluminum body. PRON VERB DET NOUN NOUN PUNCT I liked aluminum body . [],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]',\n",
       " 'screen Lightweight and the screen is beautiful! ADV CCONJ DET NOUN AUX ADJ PUNCT Lightweight screen beautiful ! [0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[]',\n",
       " 'build quality From the build quality to the performance, everything about it has been sub-par from what I would have expected from Apple. ADP DET NOUN NOUN ADP DET NOUN PUNCT PRON ADP PRON AUX AUX ADJ ADJ ADJ ADP PRON PRON VERB AUX VERB ADP PROPN PUNCT From build quality performance , everything sub-par I would expected Apple . [],[],[0.375, 0.0, 0.625],[0.125, 0.0, 0.875],[],[],[],[],[],[],[0.0, 0.0, 1.0],[]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(LAPTOP_Test)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(0,L):\n",
    "    X_test.append(LAPTOP_Test[i][0] + \" \" + LAPTOP_Test[i][1] + \" \" + LAPTOP_Test[i][2] + \" \" + LAPTOP_Test[i][3]+ \" \" + LAPTOP_Test[i][4])\n",
    "    y_test.append(label_test[i])\n",
    "\n",
    "X_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cord I charge it at night and skip taking the cord with me because of the good battery life. PRON VERB PRON ADP NOUN CCONJ NOUN VERB DET NOUN ADP PRON SCONJ ADP DET PROPN PROPN NOUN PUNCT I charge night skip taking cord good battery life . [],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]',\n",
       " 'battery life I charge it at night and skip taking the cord with me because of the good battery life. PRON VERB PRON ADP NOUN CCONJ NOUN VERB DET NOUN ADP PRON SCONJ ADP DET PROPN PROPN NOUN PUNCT I charge night skip taking cord good battery life . [],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]',\n",
       " 'service center The tech guy then said the service center does not do 1-to-1 exchange and I have to direct my concern to the \"sales\" team, which is the retail shop which I bought my netbook from. DET NOUN NOUN ADV VERB DET NOUN NOUN AUX PART AUX NUM NOUN CCONJ PRON AUX PART VERB DET NOUN ADP DET PUNCT NOUN PUNCT NOUN PUNCT DET AUX DET ADJ NOUN DET PRON VERB DET NOUN ADP PUNCT The tech guy said service center 1-to-1 exchange I direct concern `` sales \\'\\' team , retail shop I bought netbook . [],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[],[]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(LAPTOP_Train)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(0,L):\n",
    "    X_train.append(LAPTOP_Train[i][0] + \" \" + LAPTOP_Train[i][1] + \" \" + LAPTOP_Train[i][2] + \" \" + LAPTOP_Train[i][3]+ \" \" + LAPTOP_Train[i][4])\n",
    "    y_train.append(label_train[i])\n",
    "\n",
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation avec TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2313, 3079)\n",
      "(2313, 3079)\n",
      "(49, 3079)\n",
      "(49, 3079)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Create feature vectors\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "print(X_test_counts.shape)\n",
    "print(X_test_tf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le nombre de features est toujours le même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.087121s; Prediction time: 0.019997s\n",
      "Accuracy total score : 0.8367346938775511\n",
      "positive:  {'precision': 0.8666666666666667, 'recall': 0.896551724137931, 'f1-score': 0.8813559322033899, 'support': 29}\n",
      "negative:  {'precision': 0.7647058823529411, 'recall': 0.8125, 'f1-score': 0.787878787878788, 'support': 16}\n",
      "neutral:  {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 4}\n",
      "------ REPORT ------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.7647058823529411,\n",
       "  'recall': 0.8125,\n",
       "  'f1-score': 0.787878787878788,\n",
       "  'support': 16},\n",
       " 'neutral': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 4},\n",
       " 'positive': {'precision': 0.8666666666666667,\n",
       "  'recall': 0.896551724137931,\n",
       "  'f1-score': 0.8813559322033899,\n",
       "  'support': 29},\n",
       " 'accuracy': 0.8367346938775511,\n",
       " 'macro avg': {'precision': 0.8771241830065359,\n",
       "  'recall': 0.7363505747126436,\n",
       "  'f1-score': 0.7786337955829481,\n",
       "  'support': 49},\n",
       " 'weighted avg': {'precision': 0.8442577030812325,\n",
       "  'recall': 0.8367346938775511,\n",
       "  'f1-score': 0.8333071287066445,\n",
       "  'support': 49}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test avec kernel=rbf\n",
    "classifier_linear = svm.SVC(kernel='rbf')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(X_train_tf, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(X_test_tf)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(y_test, prediction_linear, output_dict=True)\n",
    "\n",
    "print(\"Accuracy total score :\", accuracy_score(y_test, prediction_linear))\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n",
    "print('------ REPORT ------')\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont déjà très bon, avec une précision de 0.88 pour les **positif**, 1.0 pour les **negatif** et 0.87 pour les **neutre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.221312s; Prediction time: 0.018000s\n",
      "Accuracy total score : 0.9183673469387755\n",
      "positive:  {'precision': 0.9032258064516129, 'recall': 0.9655172413793104, 'f1-score': 0.9333333333333333, 'support': 29}\n",
      "negative:  {'precision': 0.9333333333333333, 'recall': 0.875, 'f1-score': 0.9032258064516129, 'support': 16}\n",
      "neutral:  {'precision': 1.0, 'recall': 0.75, 'f1-score': 0.8571428571428571, 'support': 4}\n",
      "------ REPORT ------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.9333333333333333,\n",
       "  'recall': 0.875,\n",
       "  'f1-score': 0.9032258064516129,\n",
       "  'support': 16},\n",
       " 'neutral': {'precision': 1.0,\n",
       "  'recall': 0.75,\n",
       "  'f1-score': 0.8571428571428571,\n",
       "  'support': 4},\n",
       " 'positive': {'precision': 0.9032258064516129,\n",
       "  'recall': 0.9655172413793104,\n",
       "  'f1-score': 0.9333333333333333,\n",
       "  'support': 29},\n",
       " 'accuracy': 0.9183673469387755,\n",
       " 'macro avg': {'precision': 0.9455197132616487,\n",
       "  'recall': 0.8635057471264368,\n",
       "  'f1-score': 0.8979006656426011,\n",
       "  'support': 49},\n",
       " 'weighted avg': {'precision': 0.920956769804696,\n",
       "  'recall': 0.9183673469387755,\n",
       "  'f1-score': 0.9172826734380388,\n",
       "  'support': 49}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test avec kernel=poly\n",
    "classifier_linear = svm.SVC(kernel='poly')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(X_train_tf, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(X_test_tf)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(y_test, prediction_linear, output_dict=True)\n",
    "\n",
    "print(\"Accuracy total score :\", accuracy_score(y_test, prediction_linear))\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n",
    "print('------ REPORT ------')\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir testé des kernels sigmoid et linéaire également, le noyau Polynomial semble donner de meilleur résultat que ceux-ci, ainsi que le RBF précédemment, notamment pour les positifs et les neutres. \n",
    "Je garde donc cette typologie avec le kernel POLY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petit plus \n",
    "------------------------------------------------------------------- \n",
    "\n",
    "### NEURAL NETWORK : FULLY CONNECTED LAYERS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " ...\n",
      " [ 0  0  0 ... 20  1 21]\n",
      " [ 0  0  0 ...  1  1 24]\n",
      " [ 0  0  0 ...  1  1 24]]\n",
      "(2313, 200)\n",
      "(2313,)\n",
      "(2313, 3)\n",
      "[[0 0 0 ... 1 2 1]\n",
      " [0 0 0 ... 1 2 1]\n",
      " [0 0 0 ... 1 2 1]\n",
      " ...\n",
      " [0 0 0 ... 1 2 1]\n",
      " [0 0 0 ... 1 2 1]\n",
      " [0 0 0 ... 1 2 1]]\n",
      "(49, 200)\n",
      "(49,)\n",
      "(49, 3)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 200\n",
    "\n",
    "# --------- TRAIN ------------ #\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_vect = pad_sequences(sequences_train,maxlen=max_len)\n",
    "print(X_train_vect)\n",
    "print(X_train_vect.shape)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y_train = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_Y_train)\n",
    "print(encoded_Y_train.shape)\n",
    "print(dummy_y_train.shape)\n",
    "\n",
    "# --------- TEST ------------ #\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_vect = pad_sequences(sequences_test,maxlen=max_len)\n",
    "print(X_test_vect)\n",
    "print(X_test_vect.shape)\n",
    "\n",
    "\n",
    "encoder.fit(y_test)\n",
    "encoded_Y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "print(encoded_Y_test.shape)\n",
    "print(dummy_y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 61.1463 - accuracy: 0.4272\n",
      "Epoch 2/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 15.7323 - accuracy: 0.5932\n",
      "Epoch 3/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 8.4993 - accuracy: 0.6969\n",
      "Epoch 4/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 5.9445 - accuracy: 0.7432\n",
      "Epoch 5/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 4.2849 - accuracy: 0.7821\n",
      "Epoch 6/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 3.5007 - accuracy: 0.7994\n",
      "Epoch 7/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 3.3117 - accuracy: 0.8054\n",
      "Epoch 8/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 2.4841 - accuracy: 0.8249\n",
      "Epoch 9/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 1.9621 - accuracy: 0.8301\n",
      "Epoch 10/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 1.5746 - accuracy: 0.8526\n",
      "Epoch 11/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 1.1420 - accuracy: 0.8733\n",
      "Epoch 12/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 1.0058 - accuracy: 0.8859\n",
      "Epoch 13/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.8818 - accuracy: 0.8807\n",
      "Epoch 14/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.7672 - accuracy: 0.8841\n",
      "Epoch 15/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.7003 - accuracy: 0.8824\n",
      "Epoch 16/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.8885\n",
      "Epoch 17/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.8837\n",
      "Epoch 18/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.8850\n",
      "Epoch 19/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.8776\n",
      "Epoch 20/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4766 - accuracy: 0.8785\n",
      "Epoch 21/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4168 - accuracy: 0.8885\n",
      "Epoch 22/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3847 - accuracy: 0.8885\n",
      "Epoch 23/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3618 - accuracy: 0.8923\n",
      "Epoch 24/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3739 - accuracy: 0.8928\n",
      "Epoch 25/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3353 - accuracy: 0.8967\n",
      "Epoch 26/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4071 - accuracy: 0.8716\n",
      "Epoch 27/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4751 - accuracy: 0.8556\n",
      "Epoch 28/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.8361\n",
      "Epoch 29/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4629 - accuracy: 0.8405\n",
      "Epoch 30/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3964 - accuracy: 0.8755\n",
      "Epoch 31/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4916 - accuracy: 0.8586\n",
      "Epoch 32/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.7959\n",
      "Epoch 33/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5800 - accuracy: 0.8279\n",
      "Epoch 34/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4342 - accuracy: 0.8552\n",
      "Epoch 35/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4372 - accuracy: 0.8539\n",
      "Epoch 36/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3482 - accuracy: 0.8785\n",
      "Epoch 37/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.2886 - accuracy: 0.8846\n",
      "Epoch 38/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3171 - accuracy: 0.8820\n",
      "Epoch 39/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3516 - accuracy: 0.8703\n",
      "Epoch 40/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5849 - accuracy: 0.8435\n",
      "Epoch 41/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.5453 - accuracy: 0.8383\n",
      "Epoch 42/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4840 - accuracy: 0.8370\n",
      "Epoch 43/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4265 - accuracy: 0.8547\n",
      "Epoch 44/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4577 - accuracy: 0.8444\n",
      "Epoch 45/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.5252 - accuracy: 0.8418\n",
      "Epoch 46/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.4770 - accuracy: 0.8491\n",
      "Epoch 47/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3527 - accuracy: 0.8595\n",
      "Epoch 48/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2809 - accuracy: 0.8794\n",
      "Epoch 49/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.8811\n",
      "Epoch 50/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3679 - accuracy: 0.8768\n",
      "Epoch 51/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.8474\n",
      "Epoch 52/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.8431\n",
      "Epoch 53/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.8292\n",
      "Epoch 54/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.8452\n",
      "Epoch 55/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.8461\n",
      "Epoch 56/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8617\n",
      "Epoch 57/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.3655 - accuracy: 0.8660\n",
      "Epoch 58/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3004 - accuracy: 0.8764\n",
      "Epoch 59/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.2955 - accuracy: 0.8859\n",
      "Epoch 60/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2669 - accuracy: 0.8828\n",
      "Epoch 61/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8733\n",
      "Epoch 62/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.3441 - accuracy: 0.8720\n",
      "Epoch 63/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.3551 - accuracy: 0.8642\n",
      "Epoch 64/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8699\n",
      "Epoch 65/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3606 - accuracy: 0.8586\n",
      "Epoch 66/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.8495\n",
      "Epoch 67/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.8379\n",
      "Epoch 68/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8621\n",
      "Epoch 69/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4132 - accuracy: 0.8617\n",
      "Epoch 70/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8677\n",
      "Epoch 71/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8586\n",
      "Epoch 72/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8608\n",
      "Epoch 73/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.8482\n",
      "Epoch 74/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8686\n",
      "Epoch 75/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8751\n",
      "Epoch 76/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.8794\n",
      "Epoch 77/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.8906\n",
      "Epoch 78/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8781\n",
      "Epoch 79/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8725\n",
      "Epoch 80/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8604\n",
      "Epoch 81/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8647\n",
      "Epoch 82/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8586\n",
      "Epoch 83/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.8517\n",
      "Epoch 84/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8651\n",
      "Epoch 85/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8681\n",
      "Epoch 86/150\n",
      "232/232 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.8703\n",
      "Epoch 87/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.3975 - accuracy: 0.8599\n",
      "Epoch 88/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4203 - accuracy: 0.8578\n",
      "Epoch 89/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4682 - accuracy: 0.8513\n",
      "Epoch 90/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3285 - accuracy: 0.8521\n",
      "Epoch 91/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2563 - accuracy: 0.8699\n",
      "Epoch 92/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3598 - accuracy: 0.8660\n",
      "Epoch 93/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.2672 - accuracy: 0.8642\n",
      "Epoch 94/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3088 - accuracy: 0.8647\n",
      "Epoch 95/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3354 - accuracy: 0.8651\n",
      "Epoch 96/150\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.3673 - accuracy: 0.8677\n",
      "Epoch 97/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3561 - accuracy: 0.8573\n",
      "Epoch 98/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4396 - accuracy: 0.8448\n",
      "Epoch 99/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5650 - accuracy: 0.8482\n",
      "Epoch 100/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4529 - accuracy: 0.8461\n",
      "Epoch 101/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4092 - accuracy: 0.8444\n",
      "Epoch 102/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2987 - accuracy: 0.8625\n",
      "Epoch 103/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.8686\n",
      "Epoch 104/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2997 - accuracy: 0.8712\n",
      "Epoch 105/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8668\n",
      "Epoch 106/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3250 - accuracy: 0.8651\n",
      "Epoch 107/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2548 - accuracy: 0.8716\n",
      "Epoch 108/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2358 - accuracy: 0.8651\n",
      "Epoch 109/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2830 - accuracy: 0.8617\n",
      "Epoch 110/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2980 - accuracy: 0.8707\n",
      "Epoch 111/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5413 - accuracy: 0.8361\n",
      "Epoch 112/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5531 - accuracy: 0.8340\n",
      "Epoch 113/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4538 - accuracy: 0.8448\n",
      "Epoch 114/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4086 - accuracy: 0.8521\n",
      "Epoch 115/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3131 - accuracy: 0.8565\n",
      "Epoch 116/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3117 - accuracy: 0.8513\n",
      "Epoch 117/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3717 - accuracy: 0.8547\n",
      "Epoch 118/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3107 - accuracy: 0.8612\n",
      "Epoch 119/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3531 - accuracy: 0.8578\n",
      "Epoch 120/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2961 - accuracy: 0.8599\n",
      "Epoch 121/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3819 - accuracy: 0.8595\n",
      "Epoch 122/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4857 - accuracy: 0.8530\n",
      "Epoch 123/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3018 - accuracy: 0.8586\n",
      "Epoch 124/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2838 - accuracy: 0.8681\n",
      "Epoch 125/150\n",
      "232/232 [==============================] - 1s 2ms/step - loss: 0.2402 - accuracy: 0.8712\n",
      "Epoch 126/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2653 - accuracy: 0.8668\n",
      "Epoch 127/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2357 - accuracy: 0.8699\n",
      "Epoch 128/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3331 - accuracy: 0.8729\n",
      "Epoch 129/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3496 - accuracy: 0.8552\n",
      "Epoch 130/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4310 - accuracy: 0.8470\n",
      "Epoch 131/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4422 - accuracy: 0.8487\n",
      "Epoch 132/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8504\n",
      "Epoch 133/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3810 - accuracy: 0.8552\n",
      "Epoch 134/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3862 - accuracy: 0.8530\n",
      "Epoch 135/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.8651\n",
      "Epoch 136/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4783 - accuracy: 0.8586\n",
      "Epoch 137/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4082 - accuracy: 0.8508\n",
      "Epoch 138/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3250 - accuracy: 0.8543\n",
      "Epoch 139/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3313 - accuracy: 0.8556\n",
      "Epoch 140/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2938 - accuracy: 0.8612\n",
      "Epoch 141/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3114 - accuracy: 0.8617\n",
      "Epoch 142/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3576 - accuracy: 0.8539\n",
      "Epoch 143/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4056 - accuracy: 0.8573\n",
      "Epoch 144/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3137 - accuracy: 0.8569\n",
      "Epoch 145/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3763 - accuracy: 0.8604\n",
      "Epoch 146/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3370 - accuracy: 0.8595\n",
      "Epoch 147/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3034 - accuracy: 0.8599\n",
      "Epoch 148/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2282 - accuracy: 0.8716\n",
      "Epoch 149/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2377 - accuracy: 0.8742\n",
      "Epoch 150/150\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3052 - accuracy: 0.8712\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3079 - accuracy: 0.8742\n",
      "Accuracy: 87.42\n",
      "Loss : 0.31\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=200, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_vect, dummy_y_train, epochs=150, batch_size=10)\n",
    "\n",
    "# evaluate the keras model\n",
    "loss, accuracy = model.evaluate(X_train_vect, dummy_y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print('Loss : %.2f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cela permet de montrer que SVM peut être plus performant que certains modèles complexe comme les réseaux de neurones. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
