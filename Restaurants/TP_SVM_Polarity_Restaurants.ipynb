{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP ANALYSE DE SENTIMENT - PARTIE RESTAURANTS\n",
    "\n",
    "Le jeu de données Restaurant_Train est composé de 3041 phrases en anglais \n",
    "tirées des critiques de restaurants.\n",
    "Les valeurs possibles pour la polarité des aspects sont : “positive”, “negative”, “conflict”, “neutral”. \n",
    "Les valeurs possibles des categories sont : “food”, “service”, “price”, “ambience”, “anecdotes/miscellaneous”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données Laptop_Train est composé de 3045 phrases anglaises extraites \n",
    "des commentaires des clients sur les ordinateurs portables. Des annotateurs humains \n",
    "expérimentés ont annoté les termes d'aspect des phrases et leurs polarités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première tâche : analyse des sentiments du jeux de données sur les restaurants \n",
    "On commence par télécharger les fichiers suivants qui se trouvent dans le repertoire TPSA/datasets sur Moodle :\n",
    "* Restaurants_Train.xml (ensemble d'entrainement - contient les valeurs de polarité)\n",
    "* Restaurants_Test_NoLabels.xml (ensemble de test - sans les polarités)\n",
    "* Restaurants_Test_Gold.xml (ensemble de validation - sans les polarités)\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Objectif 1: calculer la polarité des mots dans les deux jeux de données à l’aide d’un lexicon de sentiment. \n",
    "Avant d’utiliser le lexicon pour calculer la polarité des mots contenus dans les phrases dans les deux jeux de données, il est nécessaire de faire un pré-traitement des phases (negation, tokenizer, PoS tagger, et NER). \n",
    "* Vous devez implémenter un système d'extraction d'informations simple. Le texte brut de chaque phrase est subdivisée en mots à l'aide d'un tokenizer. \n",
    "* Ensuite, chaque phrase est étiquetée avec des balises de partie de discours (PoS tagger), ~~ce qui s'avérera très utile à l'étape suivante, la détection d'entités nommées (NER).~~\n",
    "\n",
    "**Remarque**\n",
    "- La NER n'est pas nécessaire ici, car nous avons déjà les terms à identifier dans le Train et le test. \n",
    "- Les étapes à suivres sont alors les suivantes :\n",
    "\n",
    "* **Chargement**\n",
    "    * Charger les données depuis les fichiers en récupérant le terme et toutes ses informations (le term lui meme, son emplacement, sa phrase, sa polarité)\n",
    "    * Transformer les phrases en token avec PosTag afin de les ajouter dans notre dataframe et d'avoir des infos supplémentaires sur la structure de la phrase.\n",
    "* **Sentiments**\n",
    "    * Une  fois  que  le  pré-traitement  des  phrases  est  terminé,  vous  pouvez  télécharger  le lexicon SentiWordNet (https://github.com/aesuli/SentiWordNet).\n",
    "    * Apres le téléchargement, vous devez identifier la polarité associé a chaque mot dans les phrases contenues dans les jeux de données (fichiers Train et Test, 4 fichiers à traiter) en utilisant le lexicon SentiWordNet:\n",
    "     - pour chaque mot (que vous avez identifié avec le tokenizer, stop words exclues) vous cherchez si le mot est present dans le lexicon. \n",
    "     - S’il est present, alors vous assignez à ce mot la polarité positive/negative associée au mot dans le lexicon ansi que le degré associé.A vous de choisir le format (balises) pour stocker ces informations, qui vous seront utiles après. \n",
    "     - S’il n’est pas present, vous pouvez passer au mot suivant.\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier.\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\belka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#SPACY\n",
    "import en_core_web_sm   # téléchargement ici : https://spacy.io/models/en\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "#AUTRES LIBRAIRIES\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### **CHARGEMENT**\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs possibles pour la polarité des aspects sont : “positive”, “negative”, “conflict”, “neutral”. \n",
    "Les valeurs possibles des categories sont : “food”, “service”, “price”, “ambience”, “anecdotes/miscellaneous”.\n",
    "* Nous ne nous intéressons qu'au polarité des terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>1ère Méthode : On fait un dataframe avec une phase par ligne</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Attribute1</th>\n",
       "      <th>Attribute2</th>\n",
       "      <th>Attribute3</th>\n",
       "      <th>Attribute4</th>\n",
       "      <th>Attribute5</th>\n",
       "      <th>Attribute6</th>\n",
       "      <th>Attribute7</th>\n",
       "      <th>Attribute8</th>\n",
       "      <th>Attribute9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>{'term': 'staff', 'polarity': 'negative', 'fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>{'term': 'food', 'polarity': 'positive', 'from...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>{'term': 'food', 'polarity': 'positive', 'from...</td>\n",
       "      <td>{'term': 'kitchen', 'polarity': 'positive', 'f...</td>\n",
       "      <td>{'term': 'menu', 'polarity': 'neutral', 'from'...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                            Comment  \\\n",
       "0  3121               But the staff was so horrible to us.   \n",
       "1  2777  To be completely fair, the only redeeming fact...   \n",
       "2  1634  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                          Attribute1  \\\n",
       "0  {'term': 'staff', 'polarity': 'negative', 'fro...   \n",
       "1  {'term': 'food', 'polarity': 'positive', 'from...   \n",
       "2  {'term': 'food', 'polarity': 'positive', 'from...   \n",
       "\n",
       "                                          Attribute2  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  {'term': 'kitchen', 'polarity': 'positive', 'f...   \n",
       "\n",
       "                                          Attribute3 Attribute4 Attribute5  \\\n",
       "0                                               None       None       None   \n",
       "1                                               None       None       None   \n",
       "2  {'term': 'menu', 'polarity': 'neutral', 'from'...       None       None   \n",
       "\n",
       "  Attribute6 Attribute7 Attribute8 Attribute9  \n",
       "0       None       None       None       None  \n",
       "1       None       None       None       None  \n",
       "2       None       None       None       None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtree = et.parse(\"Restaurants_Train.xml\")\n",
    "xroot = xtree.getroot()\n",
    "comments_restaurants = []\n",
    "attributes_restaurants = []\n",
    "\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    attribute_restaurant = []\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    comments_restaurants.append([idi,text])\n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        #print(neighbor.attrib)\n",
    "        attribute_restaurant.append(neighbor.attrib)\n",
    "    attributes_restaurants.append(attribute_restaurant)\n",
    "\n",
    "dataframe1 = pd.DataFrame(comments_restaurants)\n",
    "dataframe2 = pd.DataFrame(attributes_restaurants)\n",
    "\n",
    "test3 = pd.concat([dataframe1, dataframe2], axis=1)\n",
    "test3.columns=['ID','Comment','Attribute1','Attribute2','Attribute3','Attribute4','Attribute5','Attribute6','Attribute7','Attribute8','Attribute9']\n",
    "test3.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce format n'est pas correcte car il y a trop de valeur vide qui pourraient biaiser les résultats.\n",
    "Tentons quelque chose de plus propre.\n",
    "\n",
    "**Le but est d'analyser chaque term, et pas chaque phrase. Nous voulons la polarité du term. Cela signifie que nous devrions plutot avoir une dataframe avec les phrases répétés par ligne, chaque ligne étant un terme dans une phrase.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>2ème Méthode : Format avec répétition des phrases pour chaque terme</font>\n",
    "* Plus propre car on veut prédire la polarité d'un TERME et non d'une PHRASE, il est donc logique d'avoir un dataframe dont chaque ligne est un terme.\n",
    "* Nous garderons donc ce format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      term  polarity from   to    id  \\\n",
       "0    staff  negative    8   13  3121   \n",
       "1     food  positive   57   61  2777   \n",
       "2     food  positive    4    8  1634   \n",
       "3  kitchen  positive   55   62  1634   \n",
       "4     menu   neutral  141  145  1634   \n",
       "\n",
       "                                                text  \n",
       "0               But the staff was so horrible to us.  \n",
       "1  To be completely fair, the only redeeming fact...  \n",
       "2  The food is uniformly exceptional, with a very...  \n",
       "3  The food is uniformly exceptional, with a very...  \n",
       "4  The food is uniformly exceptional, with a very...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- TRAIN DATAFRAME ----- #\n",
    "l = []\n",
    "\n",
    "xtree = et.parse(\"Restaurants_Train.xml\")\n",
    "xroot = xtree.getroot()\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_train = pd.DataFrame(l)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pasta</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>portions</td>\n",
       "      <td>positive</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>1579</td>\n",
       "      <td>And really large portions.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  polarity from  to    id  \\\n",
       "0  appetizers  positive    8  18   813   \n",
       "1      salads  positive   23  29   813   \n",
       "2       steak  positive   49  54   813   \n",
       "3       pasta  positive   82  87   813   \n",
       "4    portions  positive   17  25  1579   \n",
       "\n",
       "                                                text  \n",
       "0  All the appetizers and salads were fabulous, t...  \n",
       "1  All the appetizers and salads were fabulous, t...  \n",
       "2  All the appetizers and salads were fabulous, t...  \n",
       "3  All the appetizers and salads were fabulous, t...  \n",
       "4                         And really large portions.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- TEST DATAFRAME ----- #\n",
    "l = []\n",
    "\n",
    "xtree = et.parse(\"Restaurants_Test_Gold.xml\")\n",
    "xroot = xtree.getroot()\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_test = pd.DataFrame(l)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Tokenization and PosTag\n",
    "Utilisation de en_core_web_sm de Spacy car NLTK renvoie une erreur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On recharge cette fois avec les posTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      term  polarity from   to    id  \\\n",
       "0    staff  negative    8   13  3121   \n",
       "1     food  positive   57   61  2777   \n",
       "2     food  positive    4    8  1634   \n",
       "3  kitchen  positive   55   62  1634   \n",
       "4     menu   neutral  141  145  1634   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "3  The food is uniformly exceptional, with a very...   \n",
       "4  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  \n",
       "3  (The, food, is, uniformly, exceptional, ,, wit...  \n",
       "4  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "xtree = et.parse(\"Restaurants_Train.xml\")\n",
    "xroot = xtree.getroot()\n",
    "\n",
    "\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    # récupération de l'id et de la phrase\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #Tokenization du text\n",
    "    tokenize_Text = nlp(text)\n",
    "\n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        neighbor.attrib['token_text'] = tokenize_Text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_train = pd.DataFrame(l)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCONJ', 'DET', 'NOUN', 'AUX', 'ADV', 'ADJ', 'ADP', 'PRON', 'PUNCT']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on vérifie que les posTag ont bien été conservé\n",
    "df_train.token_text[0]\n",
    "[X.pos_ for X in  df_train.token_text[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pasta</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>portions</td>\n",
       "      <td>positive</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>1579</td>\n",
       "      <td>And really large portions.</td>\n",
       "      <td>(And, really, large, portions, .)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  polarity from  to    id  \\\n",
       "0  appetizers  positive    8  18   813   \n",
       "1      salads  positive   23  29   813   \n",
       "2       steak  positive   49  54   813   \n",
       "3       pasta  positive   82  87   813   \n",
       "4    portions  positive   17  25  1579   \n",
       "\n",
       "                                                text  \\\n",
       "0  All the appetizers and salads were fabulous, t...   \n",
       "1  All the appetizers and salads were fabulous, t...   \n",
       "2  All the appetizers and salads were fabulous, t...   \n",
       "3  All the appetizers and salads were fabulous, t...   \n",
       "4                         And really large portions.   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  \n",
       "3  (All, the, appetizers, and, salads, were, fabu...  \n",
       "4                  (And, really, large, portions, .)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "xtree = et.parse(\"Restaurants_Test_Gold.xml\")\n",
    "xroot = xtree.getroot()\n",
    "\n",
    "\n",
    "for sentence in xroot.findall('sentence'):\n",
    "    # récupération de l'id et de la phrase\n",
    "    idi = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "    \n",
    "    #Tokenization du text\n",
    "    tokenize_Text = nlp(text)\n",
    "\n",
    "    #print(idi, text)\n",
    "    for neighbor in sentence.iter('aspectTerm'):\n",
    "        # On mets 'id' et 'text' dans le dictionnaire avec toutes les variable qui nous intéresse\n",
    "        neighbor.attrib['id'] = idi\n",
    "        neighbor.attrib['text'] = text\n",
    "        neighbor.attrib['token_text'] = tokenize_Text\n",
    "        # on stock tous les dictionnaires crées dans une liste\n",
    "        l.append(neighbor.attrib)\n",
    "df_test = pd.DataFrame(l)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### **SENTIMENTS**\n",
    "------------------------------------------------------------\n",
    "\n",
    "* Une  fois  que  le  pré-traitement  des  phrases  est  terminé,  vous  pouvez  télécharger  le lexicon SentiWordNet (https://github.com/aesuli/SentiWordNet).\n",
    "* Apres le téléchargement, vous devez identifier la polarité associé a chaque mot dans les phrases contenues dans les jeux de données (fichiers Train et Test, 4 fichiers à traiter) en utilisant le lexicon SentiWordNet:\n",
    "     - pour chaque mot (que vous avez identifié avec le tokenizer, stop words exclues) vous cherchez si le mot est present dans le lexicon. \n",
    "     - S’il est present, alors vous assignez à ce mot la polarité positive/negative associée au mot dans le lexicon ansi que le degré associé.A vous de choisir le format (balises) pour stocker ces informations, qui vous seront utiles après. \n",
    "     - S’il n’est pas present, vous pouvez passer au mot suivant.\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier. \n",
    "------------------------------------------------------------\n",
    "\n",
    "* #### Assigner la polarité chaque mot et ajouter dans la dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pasta</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  polarity from  to   id  \\\n",
       "0  appetizers  positive    8  18  813   \n",
       "1      salads  positive   23  29  813   \n",
       "2       steak  positive   49  54  813   \n",
       "3       pasta  positive   82  87  813   \n",
       "\n",
       "                                                text Score_by_word Sentiword  \\\n",
       "0  All the appetizers and salads were fabulous, t...                           \n",
       "1  All the appetizers and salads were fabulous, t...                           \n",
       "2  All the appetizers and salads were fabulous, t...                           \n",
       "3  All the appetizers and salads were fabulous, t...                           \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  \n",
       "3  (All, the, appetizers, and, salads, were, fabu...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On ajoute deux colonnes vides à nos dataframe train et test\n",
    "#Une pour le score de chaque mot\n",
    "#df_train.insert(6,'Score_by_word',\"\")\n",
    "df_test.insert(6,'Score_by_word',\"\")\n",
    "#une pour avoir chaque mot que sentiword a détécté\n",
    "#df_train.insert(7,'Sentiword',\"\")\n",
    "df_test.insert(7,'Sentiword',\"\")\n",
    "\n",
    "\n",
    "df_test.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ FONCTIONS POUR RECUPERATION DES SENTIMENTS DANS SENTIWORD NET ------ #\n",
    "\n",
    "# Convertion des tags en simple WORDNET TAGS\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#CHARGEMENT DES SENTIMENTS DETECTES\n",
    "# renvoie une liste de score positif negatif ou neutre et renvoie une liste vide si le mot ne renvoie rien depuis senti wordnet.\n",
    "def get_sentiment(word,tag):\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Prend le premier sens du mot, le plus commun\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>2846</td>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>[[0.0, 0.625, 0.375], [0.0, 0.0, 1.0], [0.75, ...</td>\n",
       "      <td>[Not, food, outstanding, ,, little, 'perks, ',...</td>\n",
       "      <td>(Not, only, was, the, food, outstanding, ,, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perks</td>\n",
       "      <td>positive</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>2846</td>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>[[0.0, 0.625, 0.375], [0.0, 0.0, 1.0], [0.75, ...</td>\n",
       "      <td>[Not, food, outstanding, ,, little, 'perks, ',...</td>\n",
       "      <td>(Not, only, was, the, food, outstanding, ,, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>orrechiete with sausage and chicken</td>\n",
       "      <td>positive</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>1458</td>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>[[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...</td>\n",
       "      <td>[Our, agreed, favorite, orrechiete, sausage, c...</td>\n",
       "      <td>(Our, agreed, favorite, is, the, orrechiete, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>waiters</td>\n",
       "      <td>positive</td>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>1458</td>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>[[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...</td>\n",
       "      <td>[Our, agreed, favorite, orrechiete, sausage, c...</td>\n",
       "      <td>(Our, agreed, favorite, is, the, orrechiete, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>1458</td>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>[[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...</td>\n",
       "      <td>[Our, agreed, favorite, orrechiete, sausage, c...</td>\n",
       "      <td>(Our, agreed, favorite, is, the, orrechiete, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  term  polarity from   to    id  \\\n",
       "0                                staff  negative    8   13  3121   \n",
       "1                                 food  positive   57   61  2777   \n",
       "2                                 food  positive    4    8  1634   \n",
       "3                              kitchen  positive   55   62  1634   \n",
       "4                                 menu   neutral  141  145  1634   \n",
       "5                                 food  positive   17   21  2846   \n",
       "6                                perks  positive   51   56  2846   \n",
       "7  orrechiete with sausage and chicken  positive   27   62  1458   \n",
       "8                              waiters  positive   76   83  1458   \n",
       "9                                meats   neutral  152  157  1458   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "3  The food is uniformly exceptional, with a very...   \n",
       "4  The food is uniformly exceptional, with a very...   \n",
       "5  Not only was the food outstanding, but the lit...   \n",
       "6  Not only was the food outstanding, but the lit...   \n",
       "7  Our agreed favorite is the orrechiete with sau...   \n",
       "8  Our agreed favorite is the orrechiete with sau...   \n",
       "9  Our agreed favorite is the orrechiete with sau...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "3  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "4  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "5  [[0.0, 0.625, 0.375], [0.0, 0.0, 1.0], [0.75, ...   \n",
       "6  [[0.0, 0.625, 0.375], [0.0, 0.0, 1.0], [0.75, ...   \n",
       "7  [[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...   \n",
       "8  [[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...   \n",
       "9  [[], [], [0.125, 0.0, 0.875], [], [0.25, 0.0, ...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "3  [The, food, uniformly, exceptional, ,, capable...   \n",
       "4  [The, food, uniformly, exceptional, ,, capable...   \n",
       "5  [Not, food, outstanding, ,, little, 'perks, ',...   \n",
       "6  [Not, food, outstanding, ,, little, 'perks, ',...   \n",
       "7  [Our, agreed, favorite, orrechiete, sausage, c...   \n",
       "8  [Our, agreed, favorite, orrechiete, sausage, c...   \n",
       "9  [Our, agreed, favorite, orrechiete, sausage, c...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  \n",
       "3  (The, food, is, uniformly, exceptional, ,, wit...  \n",
       "4  (The, food, is, uniformly, exceptional, ,, wit...  \n",
       "5  (Not, only, was, the, food, outstanding, ,, bu...  \n",
       "6  (Not, only, was, the, food, outstanding, ,, bu...  \n",
       "7  (Our, agreed, favorite, is, the, orrechiete, w...  \n",
       "8  (Our, agreed, favorite, is, the, orrechiete, w...  \n",
       "9  (Our, agreed, favorite, is, the, orrechiete, w...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ON REMPLIT LA COLONNE VIDE QU'ON A CREE \n",
    "# ---------- TRAIN DATAFRAME ---------------- #\n",
    "ps = PorterStemmer()\n",
    "i=0\n",
    "for token in df_train.text :\n",
    "    words_data= nltk.word_tokenize(str(token))\n",
    "    tokens_without_sw = [word for word in words_data if not word in stopwords.words()]\n",
    "    #print(tokens_without_sw)\n",
    "\n",
    "    pos_val = nltk.pos_tag(tokens_without_sw)\n",
    "    #print(pos_val)\n",
    "\n",
    "    scores = []\n",
    "    words_sentence = []\n",
    "\n",
    "    for (x,y) in pos_val :\n",
    "        scores.append(get_sentiment(x,y))\n",
    "        words_sentence.append(x)\n",
    "\n",
    "    df_train['Sentiword'][i] = words_sentence\n",
    "    df_train['Score_by_word'][i] = scores\n",
    "    i+=1\n",
    "\n",
    "df_train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pasta</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>portions</td>\n",
       "      <td>positive</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>1579</td>\n",
       "      <td>And really large portions.</td>\n",
       "      <td>[[], [0.625, 0.0, 0.375], [0.25, 0.125, 0.625]...</td>\n",
       "      <td>[And, really, large, portions, .]</td>\n",
       "      <td>(And, really, large, portions, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sweet lassi</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>2882</td>\n",
       "      <td>The sweet lassi was excellent as was the lamb ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[The, sweet, lassi, excellent, lamb, chettinad...</td>\n",
       "      <td>(The, sweet, lassi, was, excellent, as, was, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lamb chettinad</td>\n",
       "      <td>positive</td>\n",
       "      <td>41</td>\n",
       "      <td>55</td>\n",
       "      <td>2882</td>\n",
       "      <td>The sweet lassi was excellent as was the lamb ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[The, sweet, lassi, excellent, lamb, chettinad...</td>\n",
       "      <td>(The, sweet, lassi, was, excellent, as, was, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>garlic naan</td>\n",
       "      <td>positive</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>2882</td>\n",
       "      <td>The sweet lassi was excellent as was the lamb ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[The, sweet, lassi, excellent, lamb, chettinad...</td>\n",
       "      <td>(The, sweet, lassi, was, excellent, as, was, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rasamalai</td>\n",
       "      <td>negative</td>\n",
       "      <td>84</td>\n",
       "      <td>93</td>\n",
       "      <td>2882</td>\n",
       "      <td>The sweet lassi was excellent as was the lamb ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[The, sweet, lassi, excellent, lamb, chettinad...</td>\n",
       "      <td>(The, sweet, lassi, was, excellent, as, was, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Service</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1609</td>\n",
       "      <td>Service was quick.</td>\n",
       "      <td>[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]</td>\n",
       "      <td>[Service, quick, .]</td>\n",
       "      <td>(Service, was, quick, .)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  polarity from  to    id  \\\n",
       "0      appetizers  positive    8  18   813   \n",
       "1          salads  positive   23  29   813   \n",
       "2           steak  positive   49  54   813   \n",
       "3           pasta  positive   82  87   813   \n",
       "4        portions  positive   17  25  1579   \n",
       "5     sweet lassi  positive    4  15  2882   \n",
       "6  lamb chettinad  positive   41  55  2882   \n",
       "7     garlic naan  positive   64  75  2882   \n",
       "8       rasamalai  negative   84  93  2882   \n",
       "9         Service  positive    0   7  1609   \n",
       "\n",
       "                                                text  \\\n",
       "0  All the appetizers and salads were fabulous, t...   \n",
       "1  All the appetizers and salads were fabulous, t...   \n",
       "2  All the appetizers and salads were fabulous, t...   \n",
       "3  All the appetizers and salads were fabulous, t...   \n",
       "4                         And really large portions.   \n",
       "5  The sweet lassi was excellent as was the lamb ...   \n",
       "6  The sweet lassi was excellent as was the lamb ...   \n",
       "7  The sweet lassi was excellent as was the lamb ...   \n",
       "8  The sweet lassi was excellent as was the lamb ...   \n",
       "9                                 Service was quick.   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "1  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "3  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "4  [[], [0.625, 0.0, 0.375], [0.25, 0.125, 0.625]...   \n",
       "5  [[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....   \n",
       "6  [[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....   \n",
       "7  [[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....   \n",
       "8  [[], [0.0, 0.0, 1.0], [], [1.0, 0.0, 0.0], [0....   \n",
       "9             [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], []]   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "1  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "2  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "3  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "4                  [And, really, large, portions, .]   \n",
       "5  [The, sweet, lassi, excellent, lamb, chettinad...   \n",
       "6  [The, sweet, lassi, excellent, lamb, chettinad...   \n",
       "7  [The, sweet, lassi, excellent, lamb, chettinad...   \n",
       "8  [The, sweet, lassi, excellent, lamb, chettinad...   \n",
       "9                                [Service, quick, .]   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  \n",
       "3  (All, the, appetizers, and, salads, were, fabu...  \n",
       "4                  (And, really, large, portions, .)  \n",
       "5  (The, sweet, lassi, was, excellent, as, was, t...  \n",
       "6  (The, sweet, lassi, was, excellent, as, was, t...  \n",
       "7  (The, sweet, lassi, was, excellent, as, was, t...  \n",
       "8  (The, sweet, lassi, was, excellent, as, was, t...  \n",
       "9                           (Service, was, quick, .)  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ON REMPLIT LA COLONNE VIDE QU'ON A CREE \n",
    "# ---------- TEST DATAFRAME ---------------- #\n",
    "ps = PorterStemmer()\n",
    "i=0\n",
    "for token in df_test.text :\n",
    "    words_data= nltk.word_tokenize(str(token))\n",
    "    tokens_without_sw = [word for word in words_data if not word in stopwords.words()]\n",
    "    #print(tokens_without_sw)\n",
    "\n",
    "    pos_val = nltk.pos_tag(tokens_without_sw)\n",
    "    #print(pos_val)\n",
    "\n",
    "    scores = []\n",
    "    words_sentence = []\n",
    "\n",
    "    for (x,y) in pos_val :\n",
    "        scores.append(get_sentiment(x,y))\n",
    "        words_sentence.append(x)\n",
    "\n",
    "    df_test['Sentiword'][i] = words_sentence\n",
    "    df_test['Score_by_word'][i] = scores\n",
    "    i+=1\n",
    "\n",
    "df_test.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ajout complémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td></td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td></td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td></td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  polarity from  to   id  \\\n",
       "0  appetizers  positive    8  18  813   \n",
       "1      salads  positive   23  29  813   \n",
       "2       steak  positive   49  54  813   \n",
       "\n",
       "                                                text  \\\n",
       "0  All the appetizers and salads were fabulous, t...   \n",
       "1  All the appetizers and salads were fabulous, t...   \n",
       "2  All the appetizers and salads were fabulous, t...   \n",
       "\n",
       "                                       Score_by_word PosTag  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...          \n",
       "1  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...          \n",
       "2  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...          \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "1  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "2  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enfin, au cas où, on ajoute une colonne avec seulement les POStag(il est plus facile de retirer des colonnes que d'en ajouter plus tard)\n",
    "\n",
    "#D'abord, On ajoute une colonne vide à nos dataframes train et test\n",
    "#df_train.insert(7,'PosTag',\"\")\n",
    "df_test.insert(7,'PosTag',\"\")\n",
    "\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On remplit train avec les pos tag\n",
    "for i in range(len(df_train.PosTag)):\n",
    "    df_train['PosTag'][i] = [(X.pos_) for X in  df_train.token_text[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On remplit test avec les pos tag\n",
    "for i in range(len(df_test.PosTag)):\n",
    "    df_test['PosTag'][i] = [(X.pos_) for X in  df_test.token_text[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On a enfin nos Dataframe propres qu'on pourra modeler à volonter (vectorizer, encoder, modifier les types etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>[CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>[PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term  polarity from  to    id  \\\n",
       "0  staff  negative    8  13  3121   \n",
       "1   food  positive   57  61  2777   \n",
       "2   food  positive    4   8  1634   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...   \n",
       "1  [PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...   \n",
       "2  [DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  polarity from  to   id  \\\n",
       "0  appetizers  positive    8  18  813   \n",
       "1      salads  positive   23  29  813   \n",
       "2       steak  positive   49  54  813   \n",
       "\n",
       "                                                text  \\\n",
       "0  All the appetizers and salads were fabulous, t...   \n",
       "1  All the appetizers and salads were fabulous, t...   \n",
       "2  All the appetizers and salads were fabulous, t...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "1  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "1  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "2  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "1  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "2  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on écrit les dataframes dans un fichier car l'étape de récupération de sentiment est longue à run (7min pour l'ensemble d'entrainement)\n",
    "#df_train.to_csv('Df_Restaurants_Train.csv')\n",
    "#df_test.to_csv('Df_Restaurants_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pemettra de les lire mais ne conservera pas le type des colonnes : \n",
    "# liste, token etc.. tout devient string donc pas super il faudra refaire un traitement si on charge depuis csv :\n",
    "\n",
    "# df_test= pd.read_csv('Df_Restaurants_Test.csv')\n",
    "# df_train= pd.read_csv('Df_Restaurants_Train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce que nous allons garder dans un premier temps : \n",
    "* Variables exlpicatives/prédictives :\n",
    "    * term\n",
    "    * text\n",
    "    * Score_by_word\n",
    "    * PosTag\n",
    "    * Sentiword\n",
    "* Label à prédire :\n",
    "    * polarity\n",
    "\n",
    "En effet, les colonnes \"from\", \"to\",\"id\" ne semblent pas être des éléments déterminant pour l'analyse. La colonne \"token_text\" n'est plus utile car nous avons fait une colonne de posTag et une de text, ce qui comprend les informations que nous souhaitions de token_text. L'enlever nous permet également de diminuer les dimensions de notre dataframe, car nous allons encoder ces colonnes pour plus tard la partie vectorisation (ce qui augmentent les dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term                                               text  \\\n",
       "0  staff               But the staff was so horrible to us.   \n",
       "1   food  To be completely fair, the only redeeming fact...   \n",
       "2   food  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...   \n",
       "1  [PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...   \n",
       "2  [DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "\n",
       "                                       Score_by_word  polarity  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...  negative   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...  positive   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...  positive   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF_train\n",
    "DF_train = df_train[['term','text','PosTag','Sentiword','Score_by_word','polarity','token_text']]\n",
    "DF_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>text</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>polarity</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appetizers</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salads</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steak</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>positive</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term                                               text  \\\n",
       "0  appetizers  All the appetizers and salads were fabulous, t...   \n",
       "1      salads  All the appetizers and salads were fabulous, t...   \n",
       "2       steak  All the appetizers and salads were fabulous, t...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "1  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "2  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "1  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "2  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "\n",
       "                                       Score_by_word  polarity  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...  positive   \n",
       "1  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...  positive   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...  positive   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF_test\n",
    "DF_test = df_test[['term','text','PosTag','Sentiword','Score_by_word','polarity','token_text']]\n",
    "DF_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder en Dataframe grace à .pkl\n",
    "#df_train.to_pickle('df_train_PANDAS_DATAFRAME.pkl')  \n",
    "#df_test.to_pickle('df_test_PANDAS_DATAFRAME.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>[CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>[PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term  polarity from  to    id  \\\n",
       "0  staff  negative    8  13  3121   \n",
       "1   food  positive   57  61  2777   \n",
       "2   food  positive    4   8  1634   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...   \n",
       "1  [PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...   \n",
       "2  [DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour recharger les dataframe : \n",
    "df_train = pd.read_pickle('df_train_PANDAS_DATAFRAME.pkl')\n",
    "df_test = pd.read_pickle('df_test_PANDAS_DATAFRAME.pkl')\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Visualisation\n",
    "     - Enfin,  vous  devez  générer  une  visualisation  des  données  à  travers  des  graphiques  pour montrer combien de mots ont une polarité positive / negative dans chaque fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPARTITION POLARITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 805.,    0.,    0., 2164.,    0.,    0.,  633.,    0.,    0.,\n",
       "           91.],\n",
       "        [  18.,    0.,    0.,   68.,    0.,    0.,   10.,    0.,    0.,\n",
       "            0.]]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <a list of 2 BarContainer objects>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = DF_train['polarity']\n",
    "x2 = DF_test['polarity']\n",
    "\n",
    "colors = ['#E69F00', '#56B4E9']\n",
    "names = ['TRAIN', 'TEST']\n",
    "         \n",
    "plt.hist([x1, x2],color = colors, label=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les nombres n'étant pas très pertinent pour étudier la répartition nous regarderons plutôt le pourcentage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2164 805 633 91\n",
      "68 18 10 0\n",
      "0.5859734633089629 0.21797996209044138 0.17140536149471974 0.02464121310587598\n",
      "0.7083333333333334 0.1875 0.10416666666666667 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOMBRES\n",
    "posTrain=len(DF_train[DF_train['polarity'] == 'positive' ].index)\n",
    "negTrain=len(DF_train[DF_train['polarity'] == 'negative' ].index)\n",
    "neuTrain=len(DF_train[DF_train['polarity'] == 'neutral' ].index)\n",
    "confTrain=len(DF_train[DF_train['polarity'] == 'conflict' ].index)\n",
    "\n",
    "posTest=len(DF_test[DF_test['polarity'] == 'positive' ].index)\n",
    "negTest=len(DF_test[DF_test['polarity'] == 'negative' ].index)\n",
    "neuTest=len(DF_test[DF_test['polarity'] == 'neutral' ].index)\n",
    "confTest=len(DF_test[DF_test['polarity'] == 'conflict' ].index)\n",
    "\n",
    "print(posTrain,negTrain,neuTrain,confTrain)\n",
    "print(posTest,negTest,neuTest,confTest)\n",
    "\n",
    "# TAUX\n",
    "pos_train=len(DF_train[DF_train['polarity'] == 'positive' ].index)/len(DF_train)\n",
    "neg_train=len(DF_train[DF_train['polarity'] == 'negative' ].index)/len(DF_train)\n",
    "neu_train=len(DF_train[DF_train['polarity'] == 'neutral' ].index)/len(DF_train)\n",
    "conf_train=len(DF_train[DF_train['polarity'] == 'conflict' ].index)/len(DF_train)\n",
    "\n",
    "pos_test=len(DF_test[DF_test['polarity'] == 'positive' ].index)/len(DF_test)\n",
    "neg_test=len(DF_test[DF_test['polarity'] == 'negative' ].index)/len(DF_test)\n",
    "neu_test=len(DF_test[DF_test['polarity'] == 'neutral' ].index)/len(DF_test)\n",
    "conf_test=len(DF_test[DF_test['polarity'] == 'conflict' ].index)/len(DF_test)\n",
    "\n",
    "print(pos_train,neg_train,neu_train,conf_train)\n",
    "print(pos_test,neg_test,neu_test,conf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x158616464f0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "barWidth = 0.4\n",
    "train = [pos_train, neg_train, neu_train, conf_train]\n",
    "test = [pos_test, neg_test, neu_test, conf_test]\n",
    "r1 = range(len(train))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "plotTrain= plt.bar(r1, train, width = barWidth, color = ['red' for i in train])\n",
    "plotTest= plt.bar(r2, test, width = barWidth, color = ['blue' for i in train])\n",
    "plt.xticks([r + barWidth / 2 for r in range(len(train))], ['positive', 'negative', 'neutral','conflict'])\n",
    "plt.legend([plotTrain, plotTest], ['TRAIN', 'TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\"> Il n'y a pas de \"conflict\" dans l'ensemble de test, on ne prendra pas en compte ces valeurs. <font>**\n",
    "\n",
    "**<font color=\"green\">Les autres classes semblent bien réparties.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_train[DF_train['polarity'] == 'conflict' ].index\n",
    "# on retire les lignes concernées\n",
    "DF_train.drop(indexNames , inplace=True)\n",
    "\n",
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_test[DF_test['polarity'] == 'conflict' ].index\n",
    "# on retire les lignes concernées\n",
    "DF_test.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_train[DF_train['polarity'] == 'conflict' ].index\n",
    "\n",
    "# On récupère les indexes qui contiennent des \"conflicts\"\n",
    "indexNames = DF_test[DF_test['polarity'] == 'conflict' ].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vérification rapide**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>[CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>[PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   term  polarity from  to    id  \\\n",
       "0      0  staff  negative    8  13  3121   \n",
       "1      1   food  positive   57  61  2777   \n",
       "2      2   food  positive    4   8  1634   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...   \n",
       "1  [PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...   \n",
       "2  [DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(DF_test[DF_test['polarity'] == 'conflict' ].index))\n",
    "print(len(DF_test[DF_test['polarity'] == 'conflict' ].index))\n",
    "\n",
    "print(len(DF_train[DF_train['polarity'] == 'conflict' ].index))\n",
    "print(len(DF_train[DF_train['polarity'] == 'conflict' ].index))\n",
    "\n",
    "\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder en Dataframe grace à .pkl\n",
    "#DF_train.to_pickle('DF_train_PANDAS_DATAFRAME.pkl')  \n",
    "#DF_test.to_pickle('DF_test_PANDAS_DATAFRAME.pkl')  \n",
    "#Pour recharger les dataframe : \n",
    "DF_train = pd.read_pickle('DF_train_PANDAS_DATAFRAME.pkl')\n",
    "DF_test = pd.read_pickle('DF_test_PANDAS_DATAFRAME.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>pizza</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3359</td>\n",
       "      <td>The pizza is the best if you like thin crusted...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.75, 0.0, 0.25], [], [...</td>\n",
       "      <td>[DET, NOUN, AUX, DET, ADJ, SCONJ, PRON, SCONJ,...</td>\n",
       "      <td>[The, pizza, best, like, thin, crusted, pizza, .]</td>\n",
       "      <td>(The, pizza, is, the, best, if, you, like, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>thin crusted pizza</td>\n",
       "      <td>neutral</td>\n",
       "      <td>34</td>\n",
       "      <td>52</td>\n",
       "      <td>3359</td>\n",
       "      <td>The pizza is the best if you like thin crusted...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.75, 0.0, 0.25], [], [...</td>\n",
       "      <td>[DET, NOUN, AUX, DET, ADJ, SCONJ, PRON, SCONJ,...</td>\n",
       "      <td>[The, pizza, best, like, thin, crusted, pizza, .]</td>\n",
       "      <td>(The, pizza, is, the, best, if, you, like, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>interior decoration</td>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>3534</td>\n",
       "      <td>All the money went into the interior decoratio...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.375, 0.0, 0.625],...</td>\n",
       "      <td>[DET, DET, NOUN, VERB, ADP, DET, ADJ, NOUN, PU...</td>\n",
       "      <td>[All, money, went, interior, decoration, ,, no...</td>\n",
       "      <td>(All, the, money, went, into, the, interior, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>chefs</td>\n",
       "      <td>negative</td>\n",
       "      <td>72</td>\n",
       "      <td>77</td>\n",
       "      <td>3534</td>\n",
       "      <td>All the money went into the interior decoratio...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.375, 0.0, 0.625],...</td>\n",
       "      <td>[DET, DET, NOUN, VERB, ADP, DET, ADJ, NOUN, PU...</td>\n",
       "      <td>[All, money, went, interior, decoration, ,, no...</td>\n",
       "      <td>(All, the, money, went, into, the, interior, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>seats</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1407</td>\n",
       "      <td>The seats are uncomfortable if you are sitting...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.75, 0.25], [], [...</td>\n",
       "      <td>[DET, NOUN, AUX, ADJ, SCONJ, PRON, AUX, VERB, ...</td>\n",
       "      <td>[The, seats, uncomfortable, sitting, wall, woo...</td>\n",
       "      <td>(The, seats, are, uncomfortable, if, you, are,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>seltzer with lime</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>565</td>\n",
       "      <td>I asked for seltzer with lime, no ice.</td>\n",
       "      <td>[[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...</td>\n",
       "      <td>[PRON, VERB, ADP, NOUN, ADP, NOUN, PUNCT, DET,...</td>\n",
       "      <td>[I, asked, seltzer, lime, ,, ice, .]</td>\n",
       "      <td>(I, asked, for, seltzer, with, lime, ,, no, ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>pickles</td>\n",
       "      <td>positive</td>\n",
       "      <td>77</td>\n",
       "      <td>84</td>\n",
       "      <td>675</td>\n",
       "      <td>Don't go alone---even two people isn't enough ...</td>\n",
       "      <td>[[], [], [], [0.25, 0.25, 0.5], [], [], [], [0...</td>\n",
       "      <td>[AUX, PART, VERB, ADV, PUNCT, ADV, NUM, NOUN, ...</td>\n",
       "      <td>[Do, n't, go, alone, --, -even, two, people, n...</td>\n",
       "      <td>(Do, n't, go, alone, ---, even, two, people, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>selection of meats and seafoods</td>\n",
       "      <td>positive</td>\n",
       "      <td>91</td>\n",
       "      <td>122</td>\n",
       "      <td>675</td>\n",
       "      <td>Don't go alone---even two people isn't enough ...</td>\n",
       "      <td>[[], [], [], [0.25, 0.25, 0.5], [], [], [], [0...</td>\n",
       "      <td>[AUX, PART, VERB, ADV, PUNCT, ADV, NUM, NOUN, ...</td>\n",
       "      <td>[Do, n't, go, alone, --, -even, two, people, n...</td>\n",
       "      <td>(Do, n't, go, alone, ---, even, two, people, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                             term  polarity from   to    id  \\\n",
       "25     25                            pizza  positive    4    9  3359   \n",
       "26     26               thin crusted pizza   neutral   34   52  3359   \n",
       "27     27              interior decoration  positive   28   47  3534   \n",
       "28     28                            chefs  negative   72   77  3534   \n",
       "29     29                            seats  negative    4    9  1407   \n",
       "30     30                seltzer with lime   neutral   12   29   565   \n",
       "31     31                          pickles  positive   77   84   675   \n",
       "32     32  selection of meats and seafoods  positive   91  122   675   \n",
       "\n",
       "                                                 text  \\\n",
       "25  The pizza is the best if you like thin crusted...   \n",
       "26  The pizza is the best if you like thin crusted...   \n",
       "27  All the money went into the interior decoratio...   \n",
       "28  All the money went into the interior decoratio...   \n",
       "29  The seats are uncomfortable if you are sitting...   \n",
       "30             I asked for seltzer with lime, no ice.   \n",
       "31  Don't go alone---even two people isn't enough ...   \n",
       "32  Don't go alone---even two people isn't enough ...   \n",
       "\n",
       "                                        Score_by_word  \\\n",
       "25  [[], [0.0, 0.0, 1.0], [0.75, 0.0, 0.25], [], [...   \n",
       "26  [[], [0.0, 0.0, 1.0], [0.75, 0.0, 0.25], [], [...   \n",
       "27  [[], [0.0, 0.0, 1.0], [], [0.375, 0.0, 0.625],...   \n",
       "28  [[], [0.0, 0.0, 1.0], [], [0.375, 0.0, 0.625],...   \n",
       "29  [[], [0.0, 0.0, 1.0], [0.0, 0.75, 0.25], [], [...   \n",
       "30  [[], [], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [],...   \n",
       "31  [[], [], [], [0.25, 0.25, 0.5], [], [], [], [0...   \n",
       "32  [[], [], [], [0.25, 0.25, 0.5], [], [], [], [0...   \n",
       "\n",
       "                                               PosTag  \\\n",
       "25  [DET, NOUN, AUX, DET, ADJ, SCONJ, PRON, SCONJ,...   \n",
       "26  [DET, NOUN, AUX, DET, ADJ, SCONJ, PRON, SCONJ,...   \n",
       "27  [DET, DET, NOUN, VERB, ADP, DET, ADJ, NOUN, PU...   \n",
       "28  [DET, DET, NOUN, VERB, ADP, DET, ADJ, NOUN, PU...   \n",
       "29  [DET, NOUN, AUX, ADJ, SCONJ, PRON, AUX, VERB, ...   \n",
       "30  [PRON, VERB, ADP, NOUN, ADP, NOUN, PUNCT, DET,...   \n",
       "31  [AUX, PART, VERB, ADV, PUNCT, ADV, NUM, NOUN, ...   \n",
       "32  [AUX, PART, VERB, ADV, PUNCT, ADV, NUM, NOUN, ...   \n",
       "\n",
       "                                            Sentiword  \\\n",
       "25  [The, pizza, best, like, thin, crusted, pizza, .]   \n",
       "26  [The, pizza, best, like, thin, crusted, pizza, .]   \n",
       "27  [All, money, went, interior, decoration, ,, no...   \n",
       "28  [All, money, went, interior, decoration, ,, no...   \n",
       "29  [The, seats, uncomfortable, sitting, wall, woo...   \n",
       "30               [I, asked, seltzer, lime, ,, ice, .]   \n",
       "31  [Do, n't, go, alone, --, -even, two, people, n...   \n",
       "32  [Do, n't, go, alone, --, -even, two, people, n...   \n",
       "\n",
       "                                           token_text  \n",
       "25  (The, pizza, is, the, best, if, you, like, thi...  \n",
       "26  (The, pizza, is, the, best, if, you, like, thi...  \n",
       "27  (All, the, money, went, into, the, interior, d...  \n",
       "28  (All, the, money, went, into, the, interior, d...  \n",
       "29  (The, seats, are, uncomfortable, if, you, are,...  \n",
       "30  (I, asked, for, seltzer, with, lime, ,, no, ic...  \n",
       "31  (Do, n't, go, alone, ---, even, two, people, i...  \n",
       "32  (Do, n't, go, alone, ---, even, two, people, i...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train.reset_index(inplace=True, drop=False)\n",
    "DF_test.reset_index(inplace=True, drop=False)\n",
    "DF_train[25:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...</td>\n",
       "      <td>[CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...</td>\n",
       "      <td>[But, staff, horrible, us, .]</td>\n",
       "      <td>(But, the, staff, was, so, horrible, to, us, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...</td>\n",
       "      <td>[PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...</td>\n",
       "      <td>[To, completely, fair, ,, redeeming, factor, f...</td>\n",
       "      <td>(To, be, completely, fair, ,, the, only, redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...</td>\n",
       "      <td>[DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...</td>\n",
       "      <td>[The, food, uniformly, exceptional, ,, capable...</td>\n",
       "      <td>(The, food, is, uniformly, exceptional, ,, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   term  polarity from  to    id  \\\n",
       "0      0  staff  negative    8  13  3121   \n",
       "1      1   food  positive   57  61  2777   \n",
       "2      2   food  positive    4   8  1634   \n",
       "\n",
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [0.0, 0.625, 0.375], [],...   \n",
       "1  [[], [0.5, 0.0, 0.5], [0.625, 0.0, 0.375], [],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [], [], [0.125, 0.0,...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [CCONJ, DET, NOUN, AUX, ADV, ADJ, ADP, PRON, P...   \n",
       "1  [PART, AUX, ADV, ADJ, PUNCT, DET, ADJ, VERB, N...   \n",
       "2  [DET, NOUN, AUX, ADV, ADJ, PUNCT, ADP, DET, AD...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0                      [But, staff, horrible, us, .]   \n",
       "1  [To, completely, fair, ,, redeeming, factor, f...   \n",
       "2  [The, food, uniformly, exceptional, ,, capable...   \n",
       "\n",
       "                                          token_text  \n",
       "0    (But, the, staff, was, so, horrible, to, us, .)  \n",
       "1  (To, be, completely, fair, ,, the, only, redee...  \n",
       "2  (The, food, is, uniformly, exceptional, ,, wit...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Score_by_word</th>\n",
       "      <th>PosTag</th>\n",
       "      <th>Sentiword</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>salads</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>steak</td>\n",
       "      <td>positive</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>813</td>\n",
       "      <td>All the appetizers and salads were fabulous, t...</td>\n",
       "      <td>[[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...</td>\n",
       "      <td>[DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...</td>\n",
       "      <td>[All, appetizers, salads, fabulous, ,, steak, ...</td>\n",
       "      <td>(All, the, appetizers, and, salads, were, fabu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        term  polarity from  to   id  \\\n",
       "0      0  appetizers  positive    8  18  813   \n",
       "1      1      salads  positive   23  29  813   \n",
       "2      2       steak  positive   49  54  813   \n",
       "\n",
       "                                                text  \\\n",
       "0  All the appetizers and salads were fabulous, t...   \n",
       "1  All the appetizers and salads were fabulous, t...   \n",
       "2  All the appetizers and salads were fabulous, t...   \n",
       "\n",
       "                                       Score_by_word  \\\n",
       "0  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "1  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "2  [[], [0.0, 0.0, 1.0], [], [0.875, 0.125, 0.0],...   \n",
       "\n",
       "                                              PosTag  \\\n",
       "0  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "1  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "2  [DET, DET, NOUN, CCONJ, NOUN, AUX, ADJ, PUNCT,...   \n",
       "\n",
       "                                           Sentiword  \\\n",
       "0  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "1  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "2  [All, appetizers, salads, fabulous, ,, steak, ...   \n",
       "\n",
       "                                          token_text  \n",
       "0  (All, the, appetizers, and, salads, were, fabu...  \n",
       "1  (All, the, appetizers, and, salads, were, fabu...  \n",
       "2  (All, the, appetizers, and, salads, were, fabu...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['staff' 'But the staff was so horrible to us.'\n",
      "  'CCONJ DET NOUN AUX ADV ADJ ADP PRON PUNCT' 'But staff horrible us .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.625, 0.375],[],[]']\n",
      " ['food'\n",
      "  \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\"\n",
      "  'PART AUX ADV ADJ PUNCT DET ADJ VERB NOUN AUX DET NOUN PUNCT DET AUX ADP ADJ PUNCT CCONJ VERB PART VERB ADP ADP DET DET ADJ NOUN ADP PROPN PUNCT'\n",
      "  \"To completely fair , redeeming factor food , average , could n't make deficiencies Teodora .\"\n",
      "  '[],[0.5, 0.0, 0.5],[0.625, 0.0, 0.375],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[],[],[0.125, 0.125, 0.75],[],[]']\n",
      " ['food'\n",
      "  \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\"\n",
      "  'DET NOUN AUX ADV ADJ PUNCT ADP DET ADV ADJ NOUN DET VERB ADV VERB ADP DET PRON VERB SCONJ VERB PUNCT SCONJ PRON AUX ADP DET NOUN CCONJ PART PUNCT'\n",
      "  \"The food uniformly exceptional , capable kitchen proudly whip whatever feel like eating , whether 's menu .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[],[],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[],[],[],[],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ...\n",
      " ['vegetables'\n",
      "  'Each table has a pot of boiling water sunken into its surface, and you get platters of thin sliced meats, various vegetables, and rice and glass noodles.'\n",
      "  'DET NOUN AUX DET NOUN ADP VERB NOUN VERB ADP DET NOUN PUNCT CCONJ PRON VERB NOUN ADP ADJ ADJ NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN CCONJ NOUN NOUN PUNCT'\n",
      "  'Each table boiling water sunken surface , get platters thin sliced meats , various vegetables , rice glass noodles .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.125, 0.375, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['rice'\n",
      "  'Each table has a pot of boiling water sunken into its surface, and you get platters of thin sliced meats, various vegetables, and rice and glass noodles.'\n",
      "  'DET NOUN AUX DET NOUN ADP VERB NOUN VERB ADP DET NOUN PUNCT CCONJ PRON VERB NOUN ADP ADJ ADJ NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN CCONJ NOUN NOUN PUNCT'\n",
      "  'Each table boiling water sunken surface , get platters thin sliced meats , various vegetables , rice glass noodles .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.125, 0.375, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['glass noodles'\n",
      "  'Each table has a pot of boiling water sunken into its surface, and you get platters of thin sliced meats, various vegetables, and rice and glass noodles.'\n",
      "  'DET NOUN AUX DET NOUN ADP VERB NOUN VERB ADP DET NOUN PUNCT CCONJ PRON VERB NOUN ADP ADJ ADJ NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN CCONJ NOUN NOUN PUNCT'\n",
      "  'Each table boiling water sunken surface , get platters thin sliced meats , various vegetables , rice glass noodles .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[0.125, 0.375, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DATASET\n",
    "df_train = DF_train\n",
    "\n",
    "list_train = []\n",
    "label_train = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    pos_tag = [X.pos_ for X in  df_train.token_text[i]]\n",
    "    pos_tag_text = \" \".join(pos_tag)\n",
    "    senti_word = \" \".join(df_train.Sentiword[i])\n",
    "    score_by_word = \",\".join(str(v) for v in df_train.Score_by_word[i])\n",
    "    \n",
    "    list_train.append(\n",
    "        (df_train.term[i],\n",
    "        df_train.text[i],\n",
    "        pos_tag_text,\n",
    "        senti_word,\n",
    "        score_by_word,\n",
    "        ))\n",
    "    label_train.append(df_train.polarity[i])\n",
    "\n",
    "list_train\n",
    "\n",
    "import numpy as np\n",
    "Restaurant_Train = np.array(list_train)\n",
    "print(Restaurant_Train)\n",
    "#print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['appetizers'\n",
      "  'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!'\n",
      "  'DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT'\n",
      "  'All appetizers salads fabulous , steak mouth watering pasta delicious ! ! !'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]']\n",
      " ['salads'\n",
      "  'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!'\n",
      "  'DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT'\n",
      "  'All appetizers salads fabulous , steak mouth watering pasta delicious ! ! !'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]']\n",
      " ['steak'\n",
      "  'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!'\n",
      "  'DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT'\n",
      "  'All appetizers salads fabulous , steak mouth watering pasta delicious ! ! !'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]']\n",
      " ['pasta'\n",
      "  'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!'\n",
      "  'DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT'\n",
      "  'All appetizers salads fabulous , steak mouth watering pasta delicious ! ! !'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]']\n",
      " ['portions' 'And really large portions.' 'CCONJ ADV ADJ NOUN PUNCT'\n",
      "  'And really large portions .'\n",
      "  '[],[0.625, 0.0, 0.375],[0.25, 0.125, 0.625],[0.0, 0.0, 1.0],[]']\n",
      " ['sweet lassi'\n",
      "  'The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable.'\n",
      "  'DET ADJ PROPN AUX ADJ SCONJ AUX DET PROPN PROPN CCONJ DET PROPN NOUN CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'The sweet lassi excellent lamb chettinad garlic naan rasamalai forgettable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[1.0, 0.0, 0.0],[0.0, 0.0, 1.0],[],[],[],[],[0.5, 0.0, 0.5],[]']\n",
      " ['lamb chettinad'\n",
      "  'The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable.'\n",
      "  'DET ADJ PROPN AUX ADJ SCONJ AUX DET PROPN PROPN CCONJ DET PROPN NOUN CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'The sweet lassi excellent lamb chettinad garlic naan rasamalai forgettable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[1.0, 0.0, 0.0],[0.0, 0.0, 1.0],[],[],[],[],[0.5, 0.0, 0.5],[]']\n",
      " ['garlic naan'\n",
      "  'The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable.'\n",
      "  'DET ADJ PROPN AUX ADJ SCONJ AUX DET PROPN PROPN CCONJ DET PROPN NOUN CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'The sweet lassi excellent lamb chettinad garlic naan rasamalai forgettable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[1.0, 0.0, 0.0],[0.0, 0.0, 1.0],[],[],[],[],[0.5, 0.0, 0.5],[]']\n",
      " ['rasamalai'\n",
      "  'The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable.'\n",
      "  'DET ADJ PROPN AUX ADJ SCONJ AUX DET PROPN PROPN CCONJ DET PROPN NOUN CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'The sweet lassi excellent lamb chettinad garlic naan rasamalai forgettable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[1.0, 0.0, 0.0],[0.0, 0.0, 1.0],[],[],[],[],[0.5, 0.0, 0.5],[]']\n",
      " ['Service' 'Service was quick.' 'NOUN AUX ADJ PUNCT' 'Service quick .'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['bills'\n",
      "  \"Oh, don't even let me start with how expensive the bills were!\"\n",
      "  'INTJ PUNCT AUX PART ADV VERB PRON VERB ADP ADV ADJ DET NOUN AUX PUNCT'\n",
      "  \"Oh , n't even let start expensive bills !\"\n",
      "  '[],[],[],[0.125, 0.0, 0.875],[],[],[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[]']\n",
      " ['Service' 'Service is top notch.' 'NOUN AUX ADJ NOUN PUNCT'\n",
      "  'Service top notch .'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[]']\n",
      " ['lambc hops' 'The best thing I tasted were the lambc hops.'\n",
      "  'DET ADJ NOUN PRON VERB AUX DET PROPN VERB PUNCT'\n",
      "  'The best thing I tasted lambc hops .'\n",
      "  '[],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['seafood' 'Even though its good seafood, the prices are too high.'\n",
      "  'ADV SCONJ DET ADJ NOUN PUNCT DET NOUN AUX ADV ADJ PUNCT'\n",
      "  'Even though good seafood , prices high .'\n",
      "  '[0.125, 0.0, 0.875],[],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['prices' 'Even though its good seafood, the prices are too high.'\n",
      "  'ADV SCONJ DET ADJ NOUN PUNCT DET NOUN AUX ADV ADJ PUNCT'\n",
      "  'Even though good seafood , prices high .'\n",
      "  '[0.125, 0.0, 0.875],[],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['food'\n",
      "  'In addition, the food is very good and the prices are reasonable.'\n",
      "  'ADP NOUN PUNCT DET NOUN AUX ADV ADJ CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'In addition , food good prices reasonable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[]']\n",
      " ['prices'\n",
      "  'In addition, the food is very good and the prices are reasonable.'\n",
      "  'ADP NOUN PUNCT DET NOUN AUX ADV ADJ CCONJ DET NOUN AUX ADJ PUNCT'\n",
      "  'In addition , food good prices reasonable .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[]']\n",
      " ['food' 'The food is great and authentic.'\n",
      "  'DET NOUN AUX ADJ CCONJ ADJ PUNCT' 'The food great authentic .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[]']\n",
      " ['garlic shrimp'\n",
      "  'I recommend the garlic shrimp, okra (bindi), and anything with lamb.'\n",
      "  'PRON VERB DET ADJ NOUN PUNCT NOUN PUNCT NOUN PUNCT PUNCT CCONJ PRON ADP PROPN PUNCT'\n",
      "  'I recommend garlic shrimp , okra ( bindi ) , anything lamb .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['okra (bindi)'\n",
      "  'I recommend the garlic shrimp, okra (bindi), and anything with lamb.'\n",
      "  'PRON VERB DET ADJ NOUN PUNCT NOUN PUNCT NOUN PUNCT PUNCT CCONJ PRON ADP PROPN PUNCT'\n",
      "  'I recommend garlic shrimp , okra ( bindi ) , anything lamb .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['lamb'\n",
      "  'I recommend the garlic shrimp, okra (bindi), and anything with lamb.'\n",
      "  'PRON VERB DET ADJ NOUN PUNCT NOUN PUNCT NOUN PUNCT PUNCT CCONJ PRON ADP PROPN PUNCT'\n",
      "  'I recommend garlic shrimp , okra ( bindi ) , anything lamb .'\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['menu'\n",
      "  'The menu was impressive with selections ranging from a burger, to steak, to escargot.'\n",
      "  'DET NOUN AUX ADJ ADP NOUN VERB ADP DET NOUN PUNCT PART VERB PUNCT PART VERB PUNCT'\n",
      "  'The menu impressive selections ranging burger , steak , escargot .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['burger'\n",
      "  'The menu was impressive with selections ranging from a burger, to steak, to escargot.'\n",
      "  'DET NOUN AUX ADJ ADP NOUN VERB ADP DET NOUN PUNCT PART VERB PUNCT PART VERB PUNCT'\n",
      "  'The menu impressive selections ranging burger , steak , escargot .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['steak'\n",
      "  'The menu was impressive with selections ranging from a burger, to steak, to escargot.'\n",
      "  'DET NOUN AUX ADJ ADP NOUN VERB ADP DET NOUN PUNCT PART VERB PUNCT PART VERB PUNCT'\n",
      "  'The menu impressive selections ranging burger , steak , escargot .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['escargot'\n",
      "  'The menu was impressive with selections ranging from a burger, to steak, to escargot.'\n",
      "  'DET NOUN AUX ADJ ADP NOUN VERB ADP DET NOUN PUNCT PART VERB PUNCT PART VERB PUNCT'\n",
      "  'The menu impressive selections ranging burger , steak , escargot .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[]']\n",
      " ['breads' 'very good breads as well.' 'ADV ADJ NOUN ADV ADV PUNCT'\n",
      "  'good breads well .'\n",
      "  '[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.375, 0.0, 0.625],[]']\n",
      " ['food'\n",
      "  'Anyway, the food is good, the price is right and they have a decent wine list.'\n",
      "  'INTJ PUNCT DET NOUN AUX ADJ PUNCT DET NOUN AUX ADJ CCONJ PRON AUX DET ADJ NOUN NOUN PUNCT'\n",
      "  'Anyway , food good , price right decent wine list .'\n",
      "  '[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.875, 0.0, 0.125],[],[0.0, 0.0, 1.0],[]']\n",
      " ['price'\n",
      "  'Anyway, the food is good, the price is right and they have a decent wine list.'\n",
      "  'INTJ PUNCT DET NOUN AUX ADJ PUNCT DET NOUN AUX ADJ CCONJ PRON AUX DET ADJ NOUN NOUN PUNCT'\n",
      "  'Anyway , food good , price right decent wine list .'\n",
      "  '[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.875, 0.0, 0.125],[],[0.0, 0.0, 1.0],[]']\n",
      " ['wine list'\n",
      "  'Anyway, the food is good, the price is right and they have a decent wine list.'\n",
      "  'INTJ PUNCT DET NOUN AUX ADJ PUNCT DET NOUN AUX ADJ CCONJ PRON AUX DET ADJ NOUN NOUN PUNCT'\n",
      "  'Anyway , food good , price right decent wine list .'\n",
      "  '[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.875, 0.0, 0.125],[],[0.0, 0.0, 1.0],[]']\n",
      " ['food'\n",
      "  'The food was lousy -too sweet or too salty and the portions tiny.'\n",
      "  'DET NOUN AUX ADJ PUNCT ADJ CCONJ ADV ADJ CCONJ DET NOUN ADJ PUNCT'\n",
      "  'The food lousy -too sweet salty portions tiny .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.75, 0.25],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['portions'\n",
      "  'The food was lousy -too sweet or too salty and the portions tiny.'\n",
      "  'DET NOUN AUX ADJ PUNCT ADJ CCONJ ADV ADJ CCONJ DET NOUN ADJ PUNCT'\n",
      "  'The food lousy -too sweet salty portions tiny .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.75, 0.25],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['coconut rice' 'But the coconut rice was good.'\n",
      "  'CCONJ DET NOUN NOUN AUX ADJ PUNCT' 'But coconut rice good .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[]']\n",
      " ['Tom Kha soup' 'And the Tom Kha soup was pathetic.'\n",
      "  'CCONJ DET PROPN PROPN NOUN AUX ADJ PUNCT'\n",
      "  'And Tom Kha soup pathetic .'\n",
      "  '[],[0.125, 0.0, 0.875],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['food' 'The food now is inconsistent.' 'DET NOUN ADV AUX ADJ PUNCT'\n",
      "  'The food inconsistent .' '[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['crunchy tuna' 'Try the crunchy tuna, it is to die for.'\n",
      "  'VERB DET ADJ NOUN PUNCT PRON AUX PART VERB ADP PUNCT'\n",
      "  'Try crunchy tuna , .' '[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['prices' 'Reasonable prices.' 'ADJ NOUN PUNCT' 'Reasonable prices .'\n",
      "  '[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[]']\n",
      " ['service'\n",
      "  'Add to that great service and great food at a reasonable price and you have yourself the beginning of a great evening.'\n",
      "  'VERB ADP DET ADJ NOUN CCONJ ADJ NOUN ADP DET ADJ NOUN CCONJ PRON AUX PRON DET NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'Add great service great food reasonable price beginning great evening .'\n",
      "  '[0.625, 0.125, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['food'\n",
      "  'Add to that great service and great food at a reasonable price and you have yourself the beginning of a great evening.'\n",
      "  'VERB ADP DET ADJ NOUN CCONJ ADJ NOUN ADP DET ADJ NOUN CCONJ PRON AUX PRON DET NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'Add great service great food reasonable price beginning great evening .'\n",
      "  '[0.625, 0.125, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['price'\n",
      "  'Add to that great service and great food at a reasonable price and you have yourself the beginning of a great evening.'\n",
      "  'VERB ADP DET ADJ NOUN CCONJ ADJ NOUN ADP DET ADJ NOUN CCONJ PRON AUX PRON DET NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'Add great service great food reasonable price beginning great evening .'\n",
      "  '[0.625, 0.125, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['food'\n",
      "  'Unfortunately, the food was NOT something to get worked up about.'\n",
      "  'ADV PUNCT DET NOUN AUX ADV PRON PART AUX VERB ADP ADP PUNCT'\n",
      "  'Unfortunately , food NOT something get worked .'\n",
      "  '[0.0, 0.875, 0.125],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['food' 'The food is great and reasonably priced.'\n",
      "  'DET NOUN AUX ADJ CCONJ ADV VERB PUNCT'\n",
      "  'The food great reasonably priced .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.25, 0.625],[],[]']\n",
      " ['priced' 'The food is great and reasonably priced.'\n",
      "  'DET NOUN AUX ADJ CCONJ ADV VERB PUNCT'\n",
      "  'The food great reasonably priced .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.25, 0.625],[],[]']\n",
      " ['calzones' 'Their calzones are horrific, bad, vomit-inducing, YUCK.'\n",
      "  'DET NOUN AUX ADJ PUNCT ADJ PUNCT NOUN PUNCT NOUN PUNCT PROPN PUNCT'\n",
      "  'Their calzones horrific , bad , vomit-inducing , YUCK .'\n",
      "  '[],[],[],[],[0.0, 0.625, 0.375],[],[],[],[],[]']\n",
      " ['turnip cake'\n",
      "  'A few tips: skip the turnip cake, roast pork buns and egg custards.'\n",
      "  'DET ADJ NOUN PUNCT VERB DET NOUN NOUN PUNCT NOUN NOUN NOUN CCONJ PROPN PROPN PUNCT'\n",
      "  'A tips : skip turnip cake , roast pork buns egg custards .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['roast pork buns'\n",
      "  'A few tips: skip the turnip cake, roast pork buns and egg custards.'\n",
      "  'DET ADJ NOUN PUNCT VERB DET NOUN NOUN PUNCT NOUN NOUN NOUN CCONJ PROPN PROPN PUNCT'\n",
      "  'A tips : skip turnip cake , roast pork buns egg custards .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['egg custards'\n",
      "  'A few tips: skip the turnip cake, roast pork buns and egg custards.'\n",
      "  'DET ADJ NOUN PUNCT VERB DET NOUN NOUN PUNCT NOUN NOUN NOUN CCONJ PROPN PROPN PUNCT'\n",
      "  'A tips : skip turnip cake , roast pork buns egg custards .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['bagels' 'The bagels are fabulous.' 'DET NOUN AUX ADJ PUNCT'\n",
      "  'The bagels fabulous .' '[],[0.0, 0.0, 1.0],[0.875, 0.125, 0.0],[]']\n",
      " ['ambiance'\n",
      "  'While the ambiance and atmosphere were great, the food and service could have been a lot better.'\n",
      "  'SCONJ DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN CCONJ NOUN VERB AUX AUX DET NOUN ADJ PUNCT'\n",
      "  'While ambiance atmosphere great , food service could lot better .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.875, 0.0, 0.125],[]']\n",
      " ['atmosphere'\n",
      "  'While the ambiance and atmosphere were great, the food and service could have been a lot better.'\n",
      "  'SCONJ DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN CCONJ NOUN VERB AUX AUX DET NOUN ADJ PUNCT'\n",
      "  'While ambiance atmosphere great , food service could lot better .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.875, 0.0, 0.125],[]']\n",
      " ['food'\n",
      "  'While the ambiance and atmosphere were great, the food and service could have been a lot better.'\n",
      "  'SCONJ DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN CCONJ NOUN VERB AUX AUX DET NOUN ADJ PUNCT'\n",
      "  'While ambiance atmosphere great , food service could lot better .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.875, 0.0, 0.125],[]']\n",
      " ['service'\n",
      "  'While the ambiance and atmosphere were great, the food and service could have been a lot better.'\n",
      "  'SCONJ DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN CCONJ NOUN VERB AUX AUX DET NOUN ADJ PUNCT'\n",
      "  'While ambiance atmosphere great , food service could lot better .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.875, 0.0, 0.125],[]']\n",
      " ['waitress' 'Our waitress was sweet and accomodating, not overbearing.'\n",
      "  'DET NOUN AUX ADJ CCONJ ADJ PUNCT PART ADJ PUNCT'\n",
      "  'Our waitress sweet accomodating , overbearing .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[],[],[],[]']\n",
      " ['fish'\n",
      "  'My goodness, everything from the fish to the rice to the seaweed was absolutely amazing.'\n",
      "  'DET NOUN PUNCT PRON ADP DET NOUN ADP DET NOUN ADP DET NOUN AUX ADV ADJ PUNCT'\n",
      "  'My goodness , everything fish rice seaweed absolutely amazing .'\n",
      "  '[],[0.625, 0.0, 0.375],[],[],[],[0.0, 0.0, 1.0],[],[0.5, 0.0, 0.5],[0.5, 0.25, 0.25],[]']\n",
      " ['rice'\n",
      "  'My goodness, everything from the fish to the rice to the seaweed was absolutely amazing.'\n",
      "  'DET NOUN PUNCT PRON ADP DET NOUN ADP DET NOUN ADP DET NOUN AUX ADV ADJ PUNCT'\n",
      "  'My goodness , everything fish rice seaweed absolutely amazing .'\n",
      "  '[],[0.625, 0.0, 0.375],[],[],[],[0.0, 0.0, 1.0],[],[0.5, 0.0, 0.5],[0.5, 0.25, 0.25],[]']\n",
      " ['seaweed'\n",
      "  'My goodness, everything from the fish to the rice to the seaweed was absolutely amazing.'\n",
      "  'DET NOUN PUNCT PRON ADP DET NOUN ADP DET NOUN ADP DET NOUN AUX ADV ADJ PUNCT'\n",
      "  'My goodness , everything fish rice seaweed absolutely amazing .'\n",
      "  '[],[0.625, 0.0, 0.375],[],[],[],[0.0, 0.0, 1.0],[],[0.5, 0.0, 0.5],[0.5, 0.25, 0.25],[]']\n",
      " ['spreads'\n",
      "  'Good spreads, great beverage selections and bagels really tasty.'\n",
      "  'ADJ NOUN PUNCT ADJ NOUN NOUN CCONJ NOUN ADV ADJ PUNCT'\n",
      "  'Good spreads , great beverage selections bagels really tasty .'\n",
      "  '[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.625, 0.25, 0.125],[]']\n",
      " ['beverage selections'\n",
      "  'Good spreads, great beverage selections and bagels really tasty.'\n",
      "  'ADJ NOUN PUNCT ADJ NOUN NOUN CCONJ NOUN ADV ADJ PUNCT'\n",
      "  'Good spreads , great beverage selections bagels really tasty .'\n",
      "  '[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.625, 0.25, 0.125],[]']\n",
      " ['bagels'\n",
      "  'Good spreads, great beverage selections and bagels really tasty.'\n",
      "  'ADJ NOUN PUNCT ADJ NOUN NOUN CCONJ NOUN ADV ADJ PUNCT'\n",
      "  'Good spreads , great beverage selections bagels really tasty .'\n",
      "  '[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.625, 0.25, 0.125],[]']\n",
      " ['food options' 'The food options rule.' 'DET NOUN NOUN NOUN PUNCT'\n",
      "  'The food options rule .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.25, 0.75],[0.25, 0.0, 0.75],[]']\n",
      " ['Japanese Tapas' 'Consistently good Japanese Tapas.'\n",
      "  'ADV ADJ ADJ NOUN PUNCT' 'Consistently good Japanese Tapas .'\n",
      "  '[0.25, 0.0, 0.75],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['sushi' 'The sushi was awful!' 'DET NOUN AUX ADJ PUNCT'\n",
      "  'The sushi awful !' '[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['food' 'The food was great.' 'DET NOUN AUX ADJ PUNCT'\n",
      "  'The food great .' '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['meal'\n",
      "  'With the theater 2 blocks away we had a delicious meal in a beautiful room.'\n",
      "  'ADP DET NOUN NUM NOUN ADV PRON AUX DET ADJ NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'With theater 2 blocks away delicious meal beautiful room .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[]']\n",
      " ['room'\n",
      "  'With the theater 2 blocks away we had a delicious meal in a beautiful room.'\n",
      "  'ADP DET NOUN NUM NOUN ADV PRON AUX DET ADJ NOUN ADP DET ADJ NOUN PUNCT'\n",
      "  'With theater 2 blocks away delicious meal beautiful room .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.0, 0.0, 1.0],[]']\n",
      " ['pizza'\n",
      "  'I love the fact that the pizza tastes so good and is so cheap.'\n",
      "  'PRON VERB DET NOUN SCONJ DET NOUN VERB ADV ADJ CCONJ AUX ADV ADJ PUNCT'\n",
      "  'I love fact pizza tastes good cheap .'\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[]']\n",
      " ['Service' 'Service was prompt, friendly and great.'\n",
      "  'NOUN AUX ADJ PUNCT ADJ CCONJ ADJ PUNCT'\n",
      "  'Service prompt , friendly great .'\n",
      "  '[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['lamb chop'\n",
      "  \"I haven't eat a lamb chop as delicious as that, the salads are really nice dressed with lemon and extra virgnin olive oil.\"\n",
      "  'PRON AUX PART VERB DET NOUN NOUN ADV ADJ SCONJ DET PUNCT DET NOUN AUX ADV ADJ ADJ ADP NOUN CCONJ ADJ NOUN NOUN NOUN PUNCT'\n",
      "  \"I n't eat lamb chop delicious , salads really nice dressed lemon extra virgnin olive oil .\"\n",
      "  '[],[],[],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.875, 0.0, 0.125],[],[],[0.0, 0.5, 0.5],[],[0.0, 0.125, 0.875],[0.0, 0.125, 0.875],[]']\n",
      " ['salads'\n",
      "  \"I haven't eat a lamb chop as delicious as that, the salads are really nice dressed with lemon and extra virgnin olive oil.\"\n",
      "  'PRON AUX PART VERB DET NOUN NOUN ADV ADJ SCONJ DET PUNCT DET NOUN AUX ADV ADJ ADJ ADP NOUN CCONJ ADJ NOUN NOUN NOUN PUNCT'\n",
      "  \"I n't eat lamb chop delicious , salads really nice dressed lemon extra virgnin olive oil .\"\n",
      "  '[],[],[],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.875, 0.0, 0.125],[],[],[0.0, 0.5, 0.5],[],[0.0, 0.125, 0.875],[0.0, 0.125, 0.875],[]']\n",
      " ['service' \"Drawbacks: service is slow and they don't toast!\"\n",
      "  'NOUN PUNCT NOUN AUX ADJ CCONJ PRON AUX PART VERB PUNCT'\n",
      "  \"Drawbacks : service slow n't toast !\"\n",
      "  '[0.375, 0.125, 0.5],[],[0.0, 0.0, 1.0],[],[],[],[]']\n",
      " ['prices' 'The prices were fantastic.' 'DET NOUN AUX ADJ PUNCT'\n",
      "  'The prices fantastic .' '[],[0.0, 0.0, 1.0],[0.375, 0.0, 0.625],[]']\n",
      " ['food'\n",
      "  'The food is terrible and overall, I would have to say avoid at all costs.'\n",
      "  'DET NOUN AUX ADJ CCONJ ADJ PUNCT PRON VERB AUX PART VERB VERB ADP DET NOUN PUNCT'\n",
      "  'The food terrible overall , I would say avoid costs .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.625, 0.375],[0.0, 0.0, 1.0],[],[],[],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['food' 'The food is prepared quickly and efficiently.'\n",
      "  'DET NOUN AUX VERB ADV CCONJ ADV PUNCT'\n",
      "  'The food prepared quickly efficiently .'\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[]']\n",
      " ['food' 'The food is fresh, delicious, and reasonably priced.'\n",
      "  'DET NOUN AUX ADJ PUNCT ADJ PUNCT CCONJ ADV VERB PUNCT'\n",
      "  'The food fresh , delicious , reasonably priced .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.375, 0.625, 0.0],[],[0.75, 0.0, 0.25],[],[0.125, 0.25, 0.625],[],[]']\n",
      " ['priced' 'The food is fresh, delicious, and reasonably priced.'\n",
      "  'DET NOUN AUX ADJ PUNCT ADJ PUNCT CCONJ ADV VERB PUNCT'\n",
      "  'The food fresh , delicious , reasonably priced .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.375, 0.625, 0.0],[],[0.75, 0.0, 0.25],[],[0.125, 0.25, 0.625],[],[]']\n",
      " ['waiter'\n",
      "  \"As we were leaving, the couple standing by the door said to another waiter, we're not in a hurry.\"\n",
      "  'SCONJ PRON AUX VERB PUNCT DET NOUN VERB ADP DET NOUN VERB ADP DET NOUN PUNCT PRON AUX PART ADP DET NOUN PUNCT'\n",
      "  \"As leaving , couple standing said another waiter , 're hurry .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.0, 0.125, 0.875],[0.125, 0.0, 0.875],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['outdoor atmosphere'\n",
      "  'The outdoor atmosphere of sitting on the sidewalk watching the world go by 50 feet away on 6th avenue on a cool evening was wonderful.'\n",
      "  'DET ADJ NOUN ADP VERB ADP DET NOUN VERB DET NOUN VERB ADP NUM NOUN ADV ADP ADJ NOUN ADP DET ADJ NOUN AUX ADJ PUNCT'\n",
      "  'The outdoor atmosphere sitting sidewalk watching world go 50 feet away 6th avenue cool evening wonderful .'\n",
      "  '[],[],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[],[0.25, 0.375, 0.375],[],[0.75, 0.0, 0.25],[]']\n",
      " ['toppings' 'A large is $20, and toppings are about $3 each.'\n",
      "  'DET ADJ AUX SYM NUM PUNCT CCONJ NOUN AUX ADP SYM NUM DET PUNCT'\n",
      "  'A large $ 20 , toppings $ 3 .'\n",
      "  '[],[0.25, 0.125, 0.625],[],[],[],[0.0, 0.0, 1.0],[],[],[]']\n",
      " ['spicy tuna' \"The spicy tuna and salmon are the best we've ever had.\"\n",
      "  'DET ADJ NOUN CCONJ NOUN AUX DET ADJ PRON AUX ADV VERB PUNCT'\n",
      "  \"The spicy tuna salmon best 've ever .\"\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[]']\n",
      " ['salmon' \"The spicy tuna and salmon are the best we've ever had.\"\n",
      "  'DET ADJ NOUN CCONJ NOUN AUX DET ADJ PRON AUX ADV VERB PUNCT'\n",
      "  \"The spicy tuna salmon best 've ever .\"\n",
      "  '[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[]']\n",
      " ['food'\n",
      "  'The food was very good, a great deal, and the place its self was great.'\n",
      "  'DET NOUN AUX ADV ADJ PUNCT DET ADJ NOUN PUNCT CCONJ DET NOUN DET NOUN AUX ADJ PUNCT'\n",
      "  'The food good , great deal , place self great .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['place'\n",
      "  'The food was very good, a great deal, and the place its self was great.'\n",
      "  'DET NOUN AUX ADV ADJ PUNCT DET ADJ NOUN PUNCT CCONJ DET NOUN DET NOUN AUX ADJ PUNCT'\n",
      "  'The food good , great deal , place self great .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['food'\n",
      "  \"The food is usually good but it certainly isn't a relaxing place to go.\"\n",
      "  'DET NOUN AUX ADV ADJ CCONJ PRON ADV AUX PART DET ADJ NOUN PART VERB PUNCT'\n",
      "  \"The food usually good certainly n't relaxing place go .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.25, 0.0, 0.75],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['place'\n",
      "  \"The food is usually good but it certainly isn't a relaxing place to go.\"\n",
      "  'DET NOUN AUX ADV ADJ CCONJ PRON ADV AUX PART DET ADJ NOUN PART VERB PUNCT'\n",
      "  \"The food usually good certainly n't relaxing place go .\"\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[0.25, 0.0, 0.75],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['dinner'\n",
      "  \"After dinner, take your date to the HUGE dance floor, probably one of the biggest you'll see in NY.\"\n",
      "  'ADP NOUN PUNCT VERB DET NOUN ADP DET PROPN NOUN NOUN PUNCT ADV NUM ADP DET ADJ PRON VERB VERB ADP PROPN PUNCT'\n",
      "  \"After dinner , date HUGE dance floor , probably biggest 'll see NY .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['dance floor'\n",
      "  \"After dinner, take your date to the HUGE dance floor, probably one of the biggest you'll see in NY.\"\n",
      "  'ADP NOUN PUNCT VERB DET NOUN ADP DET PROPN NOUN NOUN PUNCT ADV NUM ADP DET ADJ PRON VERB VERB ADP PROPN PUNCT'\n",
      "  \"After dinner , date HUGE dance floor , probably biggest 'll see NY .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.125, 0.0, 0.875],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[]']\n",
      " ['server'\n",
      "  'The server was really cool and served us our food and drinks with a smile.'\n",
      "  'DET NOUN AUX ADV ADJ CCONJ VERB PRON DET NOUN CCONJ VERB ADP DET NOUN PUNCT'\n",
      "  'The server really cool served us food drinks smile .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[]']\n",
      " ['served'\n",
      "  'The server was really cool and served us our food and drinks with a smile.'\n",
      "  'DET NOUN AUX ADV ADJ CCONJ VERB PRON DET NOUN CCONJ VERB ADP DET NOUN PUNCT'\n",
      "  'The server really cool served us food drinks smile .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[]']\n",
      " ['food'\n",
      "  'The server was really cool and served us our food and drinks with a smile.'\n",
      "  'DET NOUN AUX ADV ADJ CCONJ VERB PRON DET NOUN CCONJ VERB ADP DET NOUN PUNCT'\n",
      "  'The server really cool served us food drinks smile .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[]']\n",
      " ['drinks'\n",
      "  'The server was really cool and served us our food and drinks with a smile.'\n",
      "  'DET NOUN AUX ADV ADJ CCONJ VERB PRON DET NOUN CCONJ VERB ADP DET NOUN PUNCT'\n",
      "  'The server really cool served us food drinks smile .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.625, 0.0, 0.375],[0.25, 0.125, 0.625],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[]']\n",
      " ['dishes'\n",
      "  \"The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).\"\n",
      "  'DET NOUN VERB AUX ADJ PUNCT ADV ADJ CCONJ ADJ ADP DET PROPN VERB PUNCT NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP DET ADJ PROPN NOUN NOUN PUNCT DET ADJ CCONJ ADJ PRON AUX ADV VERB PUNCT PUNCT'\n",
      "  \"The dishes offered unique , tasty fresh lamb sausages , sardines biscuits , large whole shrimp amazing pistachio ice cream ( best freshest I 've ever ) .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.625, 0.25, 0.125],[0.375, 0.625, 0.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.25, 0.125, 0.625],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.75, 0.0, 0.25],[],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['lamb sausages'\n",
      "  \"The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).\"\n",
      "  'DET NOUN VERB AUX ADJ PUNCT ADV ADJ CCONJ ADJ ADP DET PROPN VERB PUNCT NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP DET ADJ PROPN NOUN NOUN PUNCT DET ADJ CCONJ ADJ PRON AUX ADV VERB PUNCT PUNCT'\n",
      "  \"The dishes offered unique , tasty fresh lamb sausages , sardines biscuits , large whole shrimp amazing pistachio ice cream ( best freshest I 've ever ) .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.625, 0.25, 0.125],[0.375, 0.625, 0.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.25, 0.125, 0.625],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.75, 0.0, 0.25],[],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['sardines with biscuits'\n",
      "  \"The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).\"\n",
      "  'DET NOUN VERB AUX ADJ PUNCT ADV ADJ CCONJ ADJ ADP DET PROPN VERB PUNCT NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP DET ADJ PROPN NOUN NOUN PUNCT DET ADJ CCONJ ADJ PRON AUX ADV VERB PUNCT PUNCT'\n",
      "  \"The dishes offered unique , tasty fresh lamb sausages , sardines biscuits , large whole shrimp amazing pistachio ice cream ( best freshest I 've ever ) .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.625, 0.25, 0.125],[0.375, 0.625, 0.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.25, 0.125, 0.625],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.75, 0.0, 0.25],[],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['large whole shrimp'\n",
      "  \"The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).\"\n",
      "  'DET NOUN VERB AUX ADJ PUNCT ADV ADJ CCONJ ADJ ADP DET PROPN VERB PUNCT NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP DET ADJ PROPN NOUN NOUN PUNCT DET ADJ CCONJ ADJ PRON AUX ADV VERB PUNCT PUNCT'\n",
      "  \"The dishes offered unique , tasty fresh lamb sausages , sardines biscuits , large whole shrimp amazing pistachio ice cream ( best freshest I 've ever ) .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.625, 0.25, 0.125],[0.375, 0.625, 0.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.25, 0.125, 0.625],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.75, 0.0, 0.25],[],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['pistachio ice cream'\n",
      "  \"The dishes offered were unique, very tasty and fresh from the lamb sausages, sardines with biscuits, large whole shrimp to the amazing pistachio ice cream (the best and freshest I've ever had).\"\n",
      "  'DET NOUN VERB AUX ADJ PUNCT ADV ADJ CCONJ ADJ ADP DET PROPN VERB PUNCT NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP DET ADJ PROPN NOUN NOUN PUNCT DET ADJ CCONJ ADJ PRON AUX ADV VERB PUNCT PUNCT'\n",
      "  \"The dishes offered unique , tasty fresh lamb sausages , sardines biscuits , large whole shrimp amazing pistachio ice cream ( best freshest I 've ever ) .\"\n",
      "  '[],[0.0, 0.0, 1.0],[],[0.375, 0.0, 0.625],[],[0.625, 0.25, 0.125],[0.375, 0.625, 0.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.25, 0.125, 0.625],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[0.25, 0.0, 0.75],[],[0.75, 0.0, 0.25],[],[],[],[0.0, 0.0, 1.0],[],[]']\n",
      " ['office lunch' 'Went there for an office lunch.'\n",
      "  'VERB ADV ADP DET NOUN NOUN PUNCT' 'Went office lunch .'\n",
      "  '[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[]']\n",
      " ['dinner'\n",
      "  \"We've only eaten in the restaurant once, but we have ordered many times for dinner.\"\n",
      "  'PRON AUX ADV VERB ADP DET NOUN ADV PUNCT CCONJ PRON AUX VERB ADJ NOUN ADP NOUN PUNCT'\n",
      "  \"We 've eaten restaurant , ordered many times dinner .\"\n",
      "  '[],[],[],[0.0, 0.0, 1.0],[],[],[0.0, 0.0, 1.0],[0.5, 0.0, 0.5],[],[]']]\n"
     ]
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "df_test = DF_test\n",
    "\n",
    "list_test = []\n",
    "label_test = []\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    pos_tag = [X.pos_ for X in  df_test.token_text[i]]\n",
    "    pos_tag_text = \" \".join(pos_tag)\n",
    "    senti_word = \" \".join(df_test.Sentiword[i])\n",
    "    score_by_word = \",\".join(str(v) for v in df_test.Score_by_word[i])\n",
    "    \n",
    "    list_test.append(\n",
    "        (df_test.term[i],\n",
    "        df_test.text[i],\n",
    "        pos_tag_text,\n",
    "        senti_word,\n",
    "        score_by_word,\n",
    "        ))\n",
    "    label_test.append(df_test.polarity[i])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "Restaurant_Test = np.array(list_test)\n",
    "print(Restaurant_Test)\n",
    "#print(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation — TF/IDF\n",
    "\n",
    "* Aux fins de la plupart des modélisations mathématiques effectuées sur le texte et aux fins de cette expérimentation, différents processus de « vectorisation » ont été mis en oeuvre.\n",
    "\n",
    "* Le contenu textuel seul ne peut pas être modifié et contraint dans l'espace mathématique sans être transformé en nombres dans le but d'être lu par un algorithme d'apprentissage automatique.\n",
    "\n",
    "* C'est pourquoi pour les besoins des méthodes supervisées dans ce projet, différents types de vectorisation ont été utilisés pour convertir des données qualitatives en données quantitatives afin de les manipuler mathématiquement. Ces vecteurs deviennent des caractéristiques intégrées pour les modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fréquence de terme/Fréquence de document inverse (TF/IDF)**\n",
    "\n",
    "Il s'agit de la technique de vectorisation utilisée pour le modèle Support Vector Machine. \n",
    "* TF/IDF a été déployé sur les données d'entraînement avec une approche unigramme qui compte chaque mot individuel comme un terme. La « fréquence des termes » correspond à la fréquence à laquelle un certain mot apparaît dans le texte, la « fréquence inverse du document » fait référence à la réduction de la signification des mots qui apparaissent le plus souvent dans tout le texte.\n",
    "* Cela sert à faire des mots que l'on voit fréquemment dans un document donné mais pas nécessairement dans tous les documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier test avec seulement la colonne text comme variable explicative\n",
    "\n",
    "Ce premier test ne signifie pas grand chose, mais il permet de montrer l'efficacité de SVM. Il doit prédire sans savoir le term à analyser, et donc sans savoir pourquoi la même phrase peut avoir une polarité positive et parfois négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.711815s; Prediction time: 0.039871s\n",
      "positive:  {'precision': 0.868421052631579, 'recall': 0.9705882352941176, 'f1-score': 0.9166666666666667, 'support': 68}\n",
      "negative:  {'precision': 0.875, 'recall': 0.7777777777777778, 'f1-score': 0.823529411764706, 'support': 18}\n",
      "neutral:  {'precision': 0.75, 'recall': 0.3, 'f1-score': 0.4285714285714285, 'support': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(DF_train['text'])\n",
    "test_vectors = vectorizer.transform(DF_test['text'])\n",
    "\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, DF_train['polarity'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(DF_test['polarity'], prediction_linear, output_dict=True)\n",
    "\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont très intéressants malgré qu'ils ne soient pas interprétable. \n",
    "<font color='red'> Cependant ce n'est pas ce que nous cherchons. Là, notre modèle prédit positif ou négatif sans savoir le term dont il s'agit. Il n'a que les phrases pour prédire. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On doit d'abord normaliser les dimensions de nos listes. De sorte que la liste ait toujours le même nombre de features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appetizers All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!! DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT All appetizers salads fabulous , steak mouth watering pasta delicious ! ! ! [],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]',\n",
       " 'salads All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!! DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT All appetizers salads fabulous , steak mouth watering pasta delicious ! ! ! [],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]',\n",
       " 'steak All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!! DET DET NOUN CCONJ NOUN AUX ADJ PUNCT DET NOUN AUX NOUN VERB CCONJ DET NOUN AUX ADJ PUNCT PUNCT PUNCT All appetizers salads fabulous , steak mouth watering pasta delicious ! ! ! [],[0.0, 0.0, 1.0],[],[0.875, 0.125, 0.0],[],[],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[0.75, 0.0, 0.25],[],[],[]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(Restaurant_Test)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(0,L):\n",
    "    X_test.append(Restaurant_Test[i][0] + \" \" + Restaurant_Test[i][1] + \" \" + Restaurant_Test[i][2] + \" \" + Restaurant_Test[i][3]+ \" \" + Restaurant_Test[i][4])\n",
    "    y_test.append(label_test[i])\n",
    "\n",
    "X_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['staff But the staff was so horrible to us. CCONJ DET NOUN AUX ADV ADJ ADP PRON PUNCT But staff horrible us . [],[0.0, 0.0, 1.0],[0.0, 0.625, 0.375],[],[]',\n",
       " \"food To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora. PART AUX ADV ADJ PUNCT DET ADJ VERB NOUN AUX DET NOUN PUNCT DET AUX ADP ADJ PUNCT CCONJ VERB PART VERB ADP ADP DET DET ADJ NOUN ADP PROPN PUNCT To completely fair , redeeming factor food , average , could n't make deficiencies Teodora . [],[0.5, 0.0, 0.5],[0.625, 0.0, 0.375],[],[],[0.0, 0.0, 1.0],[0.0, 0.0, 1.0],[],[0.0, 0.0, 1.0],[],[],[],[],[0.125, 0.125, 0.75],[],[]\",\n",
       " \"food The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not. DET NOUN AUX ADV ADJ PUNCT ADP DET ADV ADJ NOUN DET VERB ADV VERB ADP DET PRON VERB SCONJ VERB PUNCT SCONJ PRON AUX ADP DET NOUN CCONJ PART PUNCT The food uniformly exceptional , capable kitchen proudly whip whatever feel like eating , whether 's menu . [],[0.0, 0.0, 1.0],[],[],[],[0.125, 0.0, 0.875],[0.0, 0.0, 1.0],[0.125, 0.0, 0.875],[],[],[],[],[0.0, 0.0, 1.0],[],[],[],[0.0, 0.0, 1.0],[]\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(Restaurant_Train)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(0,L):\n",
    "    X_train.append(Restaurant_Train[i][0] + \" \" + Restaurant_Train[i][1] + \" \" + Restaurant_Train[i][2] + \" \" + Restaurant_Train[i][3]+ \" \" + Restaurant_Train[i][4])\n",
    "    y_train.append(label_train[i])\n",
    "\n",
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation avec TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3693, 3784)\n",
      "(3693, 3784)\n",
      "(96, 3784)\n",
      "(96, 3784)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create feature vectors\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "print(X_test_counts.shape)\n",
    "print(X_test_tf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le nombre de features est toujours le même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.980229s; Prediction time: 0.061034s\n",
      "Accuracy total score : 0.8958333333333334\n",
      "positive:  {'precision': 0.881578947368421, 'recall': 0.9852941176470589, 'f1-score': 0.9305555555555556, 'support': 68}\n",
      "negative:  {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 18}\n",
      "neutral:  {'precision': 0.875, 'recall': 0.7, 'f1-score': 0.7777777777777777, 'support': 10}\n",
      "------ REPORT ------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 1.0,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.8,\n",
       "  'support': 18},\n",
       " 'neutral': {'precision': 0.875,\n",
       "  'recall': 0.7,\n",
       "  'f1-score': 0.7777777777777777,\n",
       "  'support': 10},\n",
       " 'positive': {'precision': 0.881578947368421,\n",
       "  'recall': 0.9852941176470589,\n",
       "  'f1-score': 0.9305555555555556,\n",
       "  'support': 68},\n",
       " 'accuracy': 0.8958333333333334,\n",
       " 'macro avg': {'precision': 0.918859649122807,\n",
       "  'recall': 0.7839869281045752,\n",
       "  'f1-score': 0.8361111111111111,\n",
       "  'support': 96},\n",
       " 'weighted avg': {'precision': 0.9030975877192983,\n",
       "  'recall': 0.8958333333333334,\n",
       "  'f1-score': 0.890162037037037,\n",
       "  'support': 96}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test avec kernel=rbf\n",
    "classifier_linear = svm.SVC(kernel='rbf')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(X_train_tf, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(X_test_tf)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(y_test, prediction_linear, output_dict=True)\n",
    "print(\"Accuracy total score :\", accuracy_score(y_test, prediction_linear))\n",
    "\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n",
    "print('------ REPORT ------')\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont déjà très bon, avec une précision de 0.88 pour les **positif**, 1.0 pour les **negatif** et 0.87 pour les **neutre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.609570s; Prediction time: 0.079969s\n",
      "Accuracy total score : 0.90625\n",
      "positive:  {'precision': 0.8933333333333333, 'recall': 0.9852941176470589, 'f1-score': 0.9370629370629371, 'support': 68}\n",
      "negative:  {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 18}\n",
      "neutral:  {'precision': 0.8888888888888888, 'recall': 0.8, 'f1-score': 0.8421052631578948, 'support': 10}\n",
      "------ REPORT ------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 1.0,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.8,\n",
       "  'support': 18},\n",
       " 'neutral': {'precision': 0.8888888888888888,\n",
       "  'recall': 0.8,\n",
       "  'f1-score': 0.8421052631578948,\n",
       "  'support': 10},\n",
       " 'positive': {'precision': 0.8933333333333333,\n",
       "  'recall': 0.9852941176470589,\n",
       "  'f1-score': 0.9370629370629371,\n",
       "  'support': 68},\n",
       " 'accuracy': 0.90625,\n",
       " 'macro avg': {'precision': 0.9274074074074073,\n",
       "  'recall': 0.8173202614379086,\n",
       "  'f1-score': 0.859722733406944,\n",
       "  'support': 96},\n",
       " 'weighted avg': {'precision': 0.9128703703703703,\n",
       "  'recall': 0.90625,\n",
       "  'f1-score': 0.9014722119985278,\n",
       "  'support': 96}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test avec kernel=poly\n",
    "classifier_linear = svm.SVC(kernel='poly')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(X_train_tf, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(X_test_tf)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(y_test, prediction_linear, output_dict=True)\n",
    "print(\"Accuracy total score :\", accuracy_score(y_test, prediction_linear))\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('neutral: ', report['neutral'])\n",
    "print('------ REPORT ------')\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir testé des kernels sigmoid et linéaire également, le noyau Polynomial semble donner de meilleur résultat que ceux-ci, ainsi que le RBF précédemment, notamment pour les positifs et les neutres. \n",
    "Je garde donc cette typologie avec le kernel POLY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petit plus \n",
    "------------------------------------------------------------------- \n",
    "\n",
    "### NEURAL NETWORK : FULLY CONNECTED LAYERS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 19  1 20]\n",
      " [ 0  0  0 ... 18  1 22]\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " ...\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  2  1]]\n",
      "(3602, 200)\n",
      "(3602,)\n",
      "(3602, 3)\n",
      "[[ 0  0  0 ...  1  1  9]\n",
      " [ 0  0  0 ...  1  1  9]\n",
      " [ 0  0  0 ...  1  1  9]\n",
      " ...\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  2  1]\n",
      " [ 0  0  0 ...  1  1 25]]\n",
      "(96, 200)\n",
      "(96,)\n",
      "(96, 3)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 200\n",
    "\n",
    "# --------- TRAIN ------------ #\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_vect = pad_sequences(sequences_train,maxlen=max_len)\n",
    "print(X_train_vect)\n",
    "print(X_train_vect.shape)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y_train = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_Y_train)\n",
    "print(encoded_Y_train.shape)\n",
    "print(dummy_y_train.shape)\n",
    "\n",
    "# --------- TEST ------------ #\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_vect = pad_sequences(sequences_test,maxlen=max_len)\n",
    "print(X_test_vect)\n",
    "print(X_test_vect.shape)\n",
    "\n",
    "\n",
    "encoder.fit(y_test)\n",
    "encoded_Y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "print(encoded_Y_test.shape)\n",
    "print(dummy_y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "361/361 [==============================] - 1s 1ms/step - loss: 59.8798 - accuracy: 0.4806\n",
      "Epoch 2/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 15.1388 - accuracy: 0.6080\n",
      "Epoch 3/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 7.6212 - accuracy: 0.6696\n",
      "Epoch 4/150\n",
      "361/361 [==============================] - 1s 2ms/step - loss: 4.7777 - accuracy: 0.7121\n",
      "Epoch 5/150\n",
      "361/361 [==============================] - 1s 1ms/step - loss: 3.2773 - accuracy: 0.7479\n",
      "Epoch 6/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 2.3684 - accuracy: 0.7662\n",
      "Epoch 7/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 1.5720 - accuracy: 0.7873\n",
      "Epoch 8/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 1.1618 - accuracy: 0.8071\n",
      "Epoch 9/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.8651 - accuracy: 0.8257\n",
      "Epoch 10/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.7994 - accuracy: 0.8120\n",
      "Epoch 11/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5646 - accuracy: 0.8426\n",
      "Epoch 12/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5119 - accuracy: 0.8418\n",
      "Epoch 13/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4988 - accuracy: 0.8443\n",
      "Epoch 14/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.8284\n",
      "Epoch 15/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.8309\n",
      "Epoch 16/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.7946\n",
      "Epoch 17/150\n",
      "361/361 [==============================] - 1s 2ms/step - loss: 0.6695 - accuracy: 0.7912\n",
      "Epoch 18/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.7862\n",
      "Epoch 19/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6247 - accuracy: 0.8032\n",
      "Epoch 20/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.7876\n",
      "Epoch 21/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5805 - accuracy: 0.7965\n",
      "Epoch 22/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.7668\n",
      "Epoch 23/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6048 - accuracy: 0.7798\n",
      "Epoch 24/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6527 - accuracy: 0.7821\n",
      "Epoch 25/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5990 - accuracy: 0.7751\n",
      "Epoch 26/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6812 - accuracy: 0.7712\n",
      "Epoch 27/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5910 - accuracy: 0.7782\n",
      "Epoch 28/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5546 - accuracy: 0.7885\n",
      "Epoch 29/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.7937\n",
      "Epoch 30/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.7887\n",
      "Epoch 31/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7848\n",
      "Epoch 32/150\n",
      "361/361 [==============================] - 1s 2ms/step - loss: 0.5242 - accuracy: 0.7926\n",
      "Epoch 33/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.7779\n",
      "Epoch 34/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5507 - accuracy: 0.7857\n",
      "Epoch 35/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5427 - accuracy: 0.7879\n",
      "Epoch 36/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.7840\n",
      "Epoch 37/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6107 - accuracy: 0.7637\n",
      "Epoch 38/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7721\n",
      "Epoch 39/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7829\n",
      "Epoch 40/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5415 - accuracy: 0.7804\n",
      "Epoch 41/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4995 - accuracy: 0.7851\n",
      "Epoch 42/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.7787\n",
      "Epoch 43/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5684 - accuracy: 0.7757\n",
      "Epoch 44/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5504 - accuracy: 0.7815\n",
      "Epoch 45/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7832\n",
      "Epoch 46/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7835\n",
      "Epoch 47/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7890\n",
      "Epoch 48/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6028 - accuracy: 0.7760\n",
      "Epoch 49/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6341 - accuracy: 0.7632\n",
      "Epoch 50/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7665\n",
      "Epoch 51/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.7740\n",
      "Epoch 52/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5246 - accuracy: 0.7807\n",
      "Epoch 53/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4694 - accuracy: 0.7857\n",
      "Epoch 54/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7837\n",
      "Epoch 55/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5207 - accuracy: 0.7776\n",
      "Epoch 56/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5910 - accuracy: 0.7723\n",
      "Epoch 57/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5914 - accuracy: 0.7560\n",
      "Epoch 58/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5119 - accuracy: 0.7657\n",
      "Epoch 59/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5158 - accuracy: 0.7618\n",
      "Epoch 60/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7593\n",
      "Epoch 61/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.7637\n",
      "Epoch 62/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.7579\n",
      "Epoch 63/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5704 - accuracy: 0.7529\n",
      "Epoch 64/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7529\n",
      "Epoch 65/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7601\n",
      "Epoch 66/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5390 - accuracy: 0.7574\n",
      "Epoch 67/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7568\n",
      "Epoch 68/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.7660\n",
      "Epoch 69/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5404 - accuracy: 0.7585\n",
      "Epoch 70/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7529\n",
      "Epoch 71/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.7510\n",
      "Epoch 72/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7496\n",
      "Epoch 73/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7476\n",
      "Epoch 74/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5132 - accuracy: 0.7488\n",
      "Epoch 75/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4696 - accuracy: 0.7515\n",
      "Epoch 76/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7507\n",
      "Epoch 77/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6094 - accuracy: 0.7468\n",
      "Epoch 78/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.7432\n",
      "Epoch 79/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5421 - accuracy: 0.7426\n",
      "Epoch 80/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.7449\n",
      "Epoch 81/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6018 - accuracy: 0.7426\n",
      "Epoch 82/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7310\n",
      "Epoch 83/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5144 - accuracy: 0.7401: 0s - loss: 0.4915 - \n",
      "Epoch 84/150\n",
      "361/361 [==============================] - 1s 1ms/step - loss: 0.5045 - accuracy: 0.7390\n",
      "Epoch 85/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.7535\n",
      "Epoch 86/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.7618\n",
      "Epoch 87/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7557\n",
      "Epoch 88/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.7510\n",
      "Epoch 89/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5819 - accuracy: 0.7465\n",
      "Epoch 90/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7515\n",
      "Epoch 91/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7451\n",
      "Epoch 92/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.7438\n",
      "Epoch 93/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5244 - accuracy: 0.7385\n",
      "Epoch 94/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4860 - accuracy: 0.7443\n",
      "Epoch 95/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.7374\n",
      "Epoch 96/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5311 - accuracy: 0.7393\n",
      "Epoch 97/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7435\n",
      "Epoch 98/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.7426\n",
      "Epoch 99/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4475 - accuracy: 0.7518\n",
      "Epoch 100/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7446\n",
      "Epoch 101/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7587\n",
      "Epoch 102/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.7532\n",
      "Epoch 103/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7404\n",
      "Epoch 104/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7368\n",
      "Epoch 105/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4815 - accuracy: 0.7404\n",
      "Epoch 106/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5772 - accuracy: 0.7240\n",
      "Epoch 107/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7315\n",
      "Epoch 108/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.7326\n",
      "Epoch 109/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7365\n",
      "Epoch 110/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5145 - accuracy: 0.7315\n",
      "Epoch 111/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7410\n",
      "Epoch 112/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7435\n",
      "Epoch 113/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6577 - accuracy: 0.7379\n",
      "Epoch 114/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7415\n",
      "Epoch 115/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.7457\n",
      "Epoch 116/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.7474\n",
      "Epoch 117/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.7474\n",
      "Epoch 118/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.7488\n",
      "Epoch 119/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5469 - accuracy: 0.7460\n",
      "Epoch 120/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7471\n",
      "Epoch 121/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5227 - accuracy: 0.7460\n",
      "Epoch 122/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7468\n",
      "Epoch 123/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.7546\n",
      "Epoch 124/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7451\n",
      "Epoch 125/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7493\n",
      "Epoch 126/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7432\n",
      "Epoch 127/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4854 - accuracy: 0.7482\n",
      "Epoch 128/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.7499: 0s - loss: 0.4815 - accuracy: 0.\n",
      "Epoch 129/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.6344 - accuracy: 0.7451\n",
      "Epoch 130/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7449\n",
      "Epoch 131/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5354 - accuracy: 0.7429\n",
      "Epoch 132/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.7371\n",
      "Epoch 133/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4837 - accuracy: 0.7385\n",
      "Epoch 134/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7512\n",
      "Epoch 135/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.7579\n",
      "Epoch 136/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7596\n",
      "Epoch 137/150\n",
      "361/361 [==============================] - 1s 1ms/step - loss: 0.4533 - accuracy: 0.7593\n",
      "Epoch 138/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.7551\n",
      "Epoch 139/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7532\n",
      "Epoch 140/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7535\n",
      "Epoch 141/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7518\n",
      "Epoch 142/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.7576\n",
      "Epoch 143/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7518\n",
      "Epoch 144/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.7518\n",
      "Epoch 145/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7399\n",
      "Epoch 146/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4753 - accuracy: 0.7493\n",
      "Epoch 147/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5281 - accuracy: 0.7512\n",
      "Epoch 148/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.7546\n",
      "Epoch 149/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7535\n",
      "Epoch 150/150\n",
      "361/361 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7485: 0s - loss: 0.4663 - accuracy: \n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7371\n",
      "Accuracy: 73.71\n",
      "Loss : 0.46\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46017691493034363, 0.737090528011322]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=200, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_vect, dummy_y_train, epochs=150, batch_size=10)\n",
    "\n",
    "# evaluate the keras model\n",
    "loss, accuracy = model.evaluate(X_train_vect, dummy_y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print('Loss : %.2f' % loss)\n",
    "\n",
    "model.evaluate(X_train_vect,dummy_y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
